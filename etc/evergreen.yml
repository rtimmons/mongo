#####################################################
#               A note on expansions                #
#####################################################

# Expansions usually appear in the form ${key|default}
# If 'key' is found in the executor's map of currently known
# expansions, the corresponding value is used. If the key can
# not be found, the default is used.
#
# Arbitrary expansions can be specified in the YAML configuration
# files in the following places:
# - The 'expansions' field for buildvariants (branch file)
# - The 'expansions' field for distros (distros file)
#
# A number of 'built-in' expansions are also available for use; these include:
# - environment variables available on the host machine
# - 'workdir' (references the executor's work directory).
# - 'task_id' (references the task id of the task the executor is working on).
# - 'build_variant' (references the executing task's buildvariant).
# - 'config_root' (references the root directory for the executor's configuration artifacts).

stepback: true
command_type: system
pre_error_fails_task: true

# Files that match an ignore-list pattern will not trigger a build, if they're the only modified
# files in the patch.
ignore:
  - ".*"
  - "!.clang-format"
  - "!.eslintrc.yml"
  - "*.md"
  - "*.rst"
  - "*.txt"
  - "/distsrc/**"
  - "/docs/**"
  - "/etc/*.yml"
  - "!/etc/evergreen.yml"
  - "README"

## Some variables for convenience:
variables:

# Used when the tests it runs depend only on mongod, mongos, the mongo shell and the tools.
- &task_template
  name: template
  depends_on:
  - name: compile
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --help
      resmoke_jobs_max: 0  # No cap on number of jobs.

# Used for tests that invoke resmoke.py and require no additional setup.
- &task_depending_on_all_template
  <<: *task_template
  depends_on:
  - name: compile_all

- &jstestfuzz_config_vars
  num_files: 15
  num_tasks: 10
  resmoke_args: --help # resmoke_args needs to be overridden to specify one of the jstestfuzz suites
  resmoke_jobs_max: 1
  should_shuffle: false
  continue_on_failure: false
  # Terminate the function when there has been no output to stdout for 30 minutes. E.g. when something is stuck in an infinite loop.
  # resmoke.py writes the test output to logkeeper and only writes to stdout when starting the next test.
  # resmoke.py not producing output on stdout means that the test is still running and presumably not going to finish.
  # Note that timeout_secs is different from exec_timeout_secs, which applies to a task and times out regardless of whether output has been written to stdout.
  timeout_secs: 1800

# Used for tests that invoke 'resmoke.py --suites=jstestfuzz*'.
- &jstestfuzz_template
  name: jstestfuzz_template
  exec_timeout_secs: 14400 # Time out the task if it runs for more than 4 hours.
  depends_on: []
  commands:
  - func: "generate fuzzer tasks"

- &multiversion_template
  name: multiversion_template
  exec_timeout_secs: 14400 # Time out the task if it runs for more than 4 hours.
  depends_on: []
  commands:
  - func: "generate multiversion tasks"

- &libfuzzertests
  name: libfuzzertests!
  execution_tasks:
  - compile_libfuzzertests
  - libfuzzertests

- &compile_task_group_template
  name: compile_task_group_template
  max_hosts: 1
  tasks: []
  setup_task:
  - func: "apply compile expansions"
  - func: "set task expansion macros"
  teardown_task:
  - func: "attach scons config log"
  - func: "attach report"
  - func: "attach artifacts"
  - func: "kill processes"
  - func: "save mongo coredumps"
  - func: "save failed unittests"
  - func: "save unstripped dbtest"
  - func: "save hang analyzer debugger files"
  - func: "save disk statistics"
  - func: "save system resource information"
  - command: shell.exec
    type: setup
    params:
      working_dir: src
      shell: bash
      script: |
        set -o verbose
        set -o errexit

        ./buildscripts/merge_corpus.sh
  - func: "archive new corpus"
  - func: "upload new corpus"
  - func: "remove files"
    vars:
      files: >-
        src/resmoke_error_code
        src/build/scons/config.log
        src/*.gcda.gcov
        src/gcov-intermediate-files.tgz
        src/*.core src/*.mdmp
        mongo-coredumps.tgz
        src/unittest_binaries/*_test${exe}
        mongo-unittests.tgz
        src/debugger*.*
        src/mongo-hanganalyzer.tgz
        diskstats.tgz
        system-resource-info.tgz
        ${report_file|src/report.json}
        ${archive_file|src/archive.json}
  setup_group_can_fail_task: true
  setup_group:
  - func: "kill processes"
  - func: "cleanup environment"
  - func: "clear OOM messages"
  - command: manifest.load
  - func: "git get project"
  - func: "get all modified patch files"
  - func: "set task expansion macros"
  # The python virtual environment is installed in ${workdir}, which is created in
  # "set up virtualenv".
  - func: "set up virtualenv"
  - func: "upload pip requirements"
  - func: "configure evergreen api credentials"
  # NOTE: To disable the compile bypass feature, comment out the next line.
  - func: "bypass compile and fetch binaries"
  - func: "update bypass expansions"
  - func: "get buildnumber"
  - func: "set up credentials"
  - func: "fetch and build OpenSSL"
  - func: "use WiredTiger develop" # noop if ${use_wt_develop} is not "true"
  - func: "set up win mount script"
  - func: "generate compile expansions"
  teardown_group:
  - func: "umount shared scons directory"
  - func: "print OOM messages"
  - func: "cleanup environment"
  timeout:
  - func: "run hang analyzer"

- &stitch_support_task_group_template
  name: stitch_support_task_group_template
  setup_task:
    - func: "apply compile expansions"
    - func: "set task expansion macros"
  teardown_task:
  setup_group_can_fail_task: true
  setup_group:
    - func: "kill processes"
    - func: "cleanup environment"
    - command: manifest.load
    - func: "git get project"
    - func: "set task expansion macros"
    - func: "set up virtualenv"
    - func: "upload pip requirements"
    - func: "get buildnumber"
    - func: "set up win mount script"
    - func: "generate compile expansions"
  teardown_group:
    - func: "umount shared scons directory"

# List of all variants that make mongocryptd
# If a variant is listed here and has a push task, mongocryptd is pushed
- mongocryptd_variants: &mongocryptd_variants
  - enterprise-amazon2
  - enterprise-debian92-64
  - enterprise-debian10-64
  - enterprise-linux-64-amazon-ami
  - enterprise-macos
  - enterprise-rhel-62-64-bit
  - enterprise-rhel-62-64-bit-inmem
  - enterprise-rhel-62-64-bit-logv2
  - enterprise-rhel-62-64-bit-flow-control-off
  - enterprise-rhel-62-64-bit-majority-read-concern-off
  - enterprise-rhel-62-64-bit-multi-txn-oplog-entries
  - enterprise-rhel-62-64-bit-required-inmem
  - enterprise-rhel-62-64-bit-required-majority-read-concern-off
  - enterprise-rhel-67-s390x
  - enterprise-rhel-70-64-bit
  - enterprise-rhel-70-64-bit-kitchen-sink
  - enterprise-rhel-70-64-bit-libunwind
  - enterprise-rhel-71-ppc64le
  - enterprise-rhel-71-ppc64le-inmem
  - enterprise-rhel-72-s390x
  - enterprise-rhel-72-s390x-inmem
  - enterprise-rhel-80-64-bit
  - enterprise-suse12-64
  - enterprise-suse15-64
  - enterprise-suse12-s390x
  - enterprise-ubuntu-dynamic-1604-64-bit
  - enterprise-ubuntu-dynamic-1604-clang
  - enterprise-ubuntu1604-64
  - enterprise-ubuntu1604-arm64
  - enterprise-ubuntu1804-64
  - enterprise-ubuntu1804-arm64
  - enterprise-ubuntu1804-ppc64le
  - enterprise-ubuntu1804-s390x
  - ubuntu1804-debug-asan
  - ubuntu1804-debug-ubsan
  - ubuntu1804-debug-aubsan-lite
  - ubuntu1804-debug-aubsan-lite_fuzzer
  - ubuntu1804-debug-aubsan-async


# List of all variants that make mh artifacts.
# If a variant is listed here and has a push task, the mh artifacts are pushed
- mh_variants: &mh_variants
  - enterprise-debian92-64
  - enterprise-macos
  - enterprise-rhel-62-64-bit
  - enterprise-rhel-70-64-bit
  - enterprise-ubuntu1604-64
  - enterprise-ubuntu1804-64

# List of all variants that use the packages.tgz
- package_variants: &package_variants
  - amazon
  - enterprise-linux-64-amazon-ami
  - amazon2
  - enterprise-amazon2
  - debian10
  - enterprise-debian10-64
  - debian92
  - enterprise-debian92-64
  - rhel62
  - enterprise-rhel-62-64-bit
  - rhel-67-s390x
  - enterprise-rhel-67-s390x
  - rhel70
  - rhel76_compile_rhel70
  - enterprise-rhel-70-64-bit
  - rhel-72-s390x
  - enterprise-rhel-71-ppc64le
  - enterprise-rhel-72-s390x
  - rhel80
  - enterprise-rhel-80-64-bit
  - suse12
  - suse12-s390x
  - enterprise-suse12-64
  - enterprise-suse12-s390x
  - suse15
  - enterprise-suse15-64
  - ubuntu1604
  - ubuntu1604-debug
  - enterprise-ubuntu1604-64
  - enterprise-ubuntu1604-arm64
  - enterprise-ubuntu-dynamic-1604-64-bit
  - enterprise-ubuntu-dynamic-1604-clang
  - ubuntu-dynamic-1604-clang
  - ubuntu1804
  - ubuntu1804-arm64
  - ubuntu1804-debug-aubsan-async
  - ubuntu1804-s390x
  - enterprise-ubuntu1804-64
  - enterprise-ubuntu1804-arm64
  - enterprise-ubuntu1804-ppc64le
  - enterprise-ubuntu1804-s390x


#######################################
#            Functions                #
#######################################

functions:

  "remove files": &remove_files
    command: shell.exec
    params:
      script: |
        if [ -z "${files}" ]; then
          exit 0
        fi
        for file in ${files}
        do
          if [ -f "$file" ]; then
            echo "Removing file $file"
            rm -f $file
          fi
        done

  "configure evergreen api credentials": &configure_evergreen_api_credentials
    command: shell.exec
    type: test
    params:
      working_dir: src
      silent: true
      script: |
        # Create the Evergreen API credentials
        cat > .evergreen.yml <<END_OF_CREDS
        api_server_host: https://evergreen.mongodb.com/api
        api_key: "${evergreen_api_key}"
        user: "${evergreen_api_user}"
        END_OF_CREDS

  "git get project": &git_get_project
    command: git.get_project
    params:
      directory: ${git_project_directory|src}
      revisions: # for each module include revision as <module_name> : ${<module_name>_rev}
        enterprise: ${enterprise_rev}
        wtdevelop: ${wtdevelop_rev}

  "fetch artifacts": &fetch_artifacts
    command: s3.get
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      remote_file: ${project}/${build_variant}/${revision}/artifacts/${build_id}.tgz
      bucket: mciuploads
      extract_to: src

  "fetch packages": &fetch_packages
    command: s3.get
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      remote_file: ${project}/${build_variant}/${revision}/artifacts/${build_id}-packages.tgz
      bucket: mciuploads
      extract_to: src
      build_variants: *package_variants

  "fetch binaries": &fetch_binaries
    command: s3.get
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      remote_file: ${mongo_binaries}
      bucket: mciuploads
      local_file: src/mongo-binaries.tgz

  "extract binaries": &extract_binaries
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        ${decompress} mongo-binaries.tgz
        cp mongodb*/bin/* .

  "check binary version": &check_binary_version
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        mongo_binary=$(find mongodb*/bin -name mongo${exe})
        # There should only be one mongo shell
        if [ $(echo $mongo_binary | wc -w) -ne 1 ]; then
          echo "There is more than 1 extracted mongo binary: $mongo_binary"
          exit 1
        fi
        # For compile bypass we need to skip the binary version check since we can tag a commit
        # after the base commit binaries were created. This would lead to a mismatch of the binaries
        # and the version from git describe --abbrev=7 in the compile_expansions.yml.
        if [ "${is_patch}" = "true" ] && [ "${bypass_compile|false}" = "true" ]; then
          echo "Skipping binary version check since we are bypassing compile in this patch build."
          exit 0
        fi
        ${activate_virtualenv}
        bin_ver=$($python -c "import yaml; print(yaml.safe_load(open('compile_expansions.yml'))['version']);" | tr -d '[ \r\n]')
        # Due to SERVER-23810, we cannot use $mongo_binary --quiet --nodb --eval "version();"
        mongo_ver=$($mongo_binary --version | perl -pe '/version v(.*)$/; $_ = $1;' | tr -d '[ \r\n]')
        # The versions must match
        if [ "$bin_ver" != "$mongo_ver" ]; then
          echo "The mongo version is $mongo_ver, expected version is $bin_ver"
          exit 1
        fi

  "fetch corpus": &fetch_corpus
    command: s3.get
    params:
      aws_key: ${s3_access_key_id}
      aws_secret: ${s3_secret_access_key}
      remote_file: ${project}/corpus/mongo-${build_variant}-latest.tgz
      bucket: fuzzer-artifacts
      local_file: src/corpus.tgz

  "extract corpus": &extract_corpus
    command: archive.auto_extract
    params:
        path: src/corpus.tgz
        destination: src/corpus

  "archive new corpus": &archive_new_corpus
    command: archive.targz_pack
    params:
      target: corpus.tgz
      source_dir: src/corpus
      include:
        - "**"

  "upload new corpus": &upload_new_corpus
    command: s3.put
    params:
      aws_key: ${s3_access_key_id}
      aws_secret: ${s3_secret_access_key}
      local_file: corpus.tgz
      remote_file: ${project}/corpus/mongo-${build_variant}-latest.tgz
      bucket: fuzzer-artifacts
      permissions: private
      content_type: ${content_type|application/gzip}
      display_name: "Fuzzer Tests Corpus Tar Archive"
      optional: true

  "get buildnumber": &get_buildnumber
    command: keyval.inc
    params:
      key: "${build_variant}_master"
      destination: "builder_num"

  "run diskstats": &run_diskstats
    command: shell.exec
    params:
      background: true
      system_log: true
      script: |
        set -o errexit
        set -o verbose

        # On Windows we can use typeperf.exe to dump performance counters.
        if [ "Windows_NT" = "$OS" ]; then
            typeperf -qx PhysicalDisk | grep Disk | grep -v _Total > disk_counters.txt
            typeperf -cf disk_counters.txt -si 5 -o mongo-diskstats
        # Linux: iostat -t option for timestamp.
        elif iostat -tdmx > /dev/null 2>&1; then
            iostat -tdmx 5 > mongo-diskstats
        # OSX: Simulate the iostat timestamp.
        elif iostat -d > /dev/null 2>&1; then
            iostat -d -w 5 | while IFS= read -r line; do printf '%s %s\n' "$(date +'%m/%d/%Y %H:%M:%S')" "$line" >> mongo-diskstats; done
        # Check if vmstat -t is available.
        elif vmstat -td  > /dev/null 2>&1; then
            vmstat -td 5 > mongo-diskstats
        # Check if vmstat -T d is available.
        elif vmstat -T d > /dev/null 2>&1; then
            vmstat -T d 5 > mongo-diskstats
        else
            printf "Cannot collect mongo-diskstats on this platform\n"
        fi

  "collect system resource info": &collect_system_resource_info
    command: shell.exec
    params:
      working_dir: src
      background: true
      system_log: true
      script: |
        ${activate_virtualenv}
        $python buildscripts/collect_resource_info.py -o system_resource_info.json -i 5

  # Run a monitor process as a background, system task to periodically
  # display how many threads interesting processes are using.
  "monitor process threads": &monitor_process_threads
    command: shell.exec
    params:
      background: true
      system_log: true
      script: |
        proc_list="(bsondump|java|lein|mongo|python|_test$|_test\.exe$)"
        if [ "Windows_NT" = "$OS" ]; then
          get_pids() {
            proc_pids=$(tasklist /fo:csv |
                        awk -F'","' '{x=$1; gsub("\"","",x); print $2, x}' |
                        grep -iE $1 |
                        cut -f1 -d ' ');
          }
          get_process_info() {
            proc_name="";
            proc_info=$(wmic process where "ProcessId=\"$1\"" get "Name,ProcessId,ThreadCount" /format:csv 2> /dev/null | grep $1);
            if [ ! -z $proc_info ]; then
              proc_name=$(echo $proc_info | cut -f2 -d ',');
              proc_threads=$(echo $proc_info | cut -f4 -d ',');
            fi;
          }
        else
          get_pids() { proc_pids=$(pgrep $1); }
          get_process_info() {
            proc_name=$(ps -p $1 -o comm=);
            # /proc is available on Linux platforms
            if [ -f /proc/$1/status ]; then
              ${set_sudo}
              proc_threads=$($sudo grep Threads /proc/$1/status | sed "s/\s//g" | cut -f2 -d ":");
            else
              proc_threads=$(ps -AM $1 | grep -vc PID);
            fi;
          }
        fi
        while [ 1 ]
        do
          get_pids $proc_list
          if [ ! -z "$proc_pids" ]; then
            printf "Running process/thread counter\n"
            printf "PROCESS\tPID\tTHREADS\n"
          fi
          for pid in $proc_pids
          do
            get_process_info $pid
            if [ ! -z "$proc_name" ]; then
              printf "$proc_name\t$pid\t$proc_threads\n"
            fi
          done
          sleep 60
        done

  "set up credentials": &set_up_credentials
    command: shell.exec
    params:
      working_dir: src
      silent: true
      script: |
        cat > mci.buildlogger <<END_OF_CREDS
        slavename='${slave}'
        passwd='${passwd}'
        builder='MCI_${build_variant}'
        build_num=${builder_num}
        build_phase='${task_name}_${execution}'
        END_OF_CREDS

  "set up win mount script": &set_up_win_mount_script
    command: shell.exec
    params:
      working_dir: src
      shell: bash
      silent: true
      script: |
        cat <<EOF > win_mount.sh
        net use X: '\\\\${win_scons_endpoint}\\share' /USER:"wincache.build.com\${win_scons_user}" '${win_scons_pass}'
        EOF
        chmod +x win_mount.sh

  "set up notary client credentials": &set_up_notary_client_credentials
    command: shell.exec
    params:
      working_dir: src
      silent: true
      script: |
        set -o errexit

        cat <<EOF > notary_env.sh
        export NOTARY_TOKEN=${signing_auth_token_44}
        EOF

        echo "${signing_auth_token_44}" > signing_auth_token

  "set up remote credentials": &set_up_remote_credentials
    command: shell.exec
    params:
      silent: true
      script: |
        set -o errexit

        # Since the macros 'private_key_remote' and 'private_key_file' are not always defined
        # we default to /dev/null to avoid syntax errors of an empty expansion.
        if [ ! -z "${private_key_remote}" ] && [ ! -z "${private_key_file}" ] ; then
          mkdir -p ~/.ssh
          echo -n "${private_key_remote}" > ${private_key_file|/dev/null}
          chmod 0600 ${private_key_file|/dev/null}
        fi

        # Ensure a clean aws configuration state
        rm -rf ~/.aws
        mkdir -p ~/.aws

        # If ${aws_profile_remote} is not specified then the config & credentials are
        # stored in the 'default' profile.
        aws_profile="${aws_profile_remote|default}"

        # The profile in the config file is specified as [profile <profile>], except
        # for [default], see http://boto3.readthedocs.io/en/latest/guide/configuration.html
        if [ $aws_profile = "default" ] ; then
          aws_profile_config="[default]"
        else
          aws_profile_config="[profile $aws_profile]"
        fi
        cat <<EOF >> ~/.aws/config
        $aws_profile_config
        region = us-east-1
        EOF

        # The profile in the credentials file is specified as [<profile>].
        cat <<EOF >> ~/.aws/credentials
        [$aws_profile]
        aws_access_key_id = ${aws_key_remote}
        aws_secret_access_key = ${aws_secret_remote}
        EOF

        cat <<EOF > ~/.boto
        [Boto]
        https_validate_certificates = False
        EOF

  "call BF Suggestion service":
    command: shell.exec
    params:
      working_dir: src
      shell: bash
      silent: true
      script: |
        report_file="report.json"
        # Check if the report file exists and has failures.
        if [ -f $report_file ] && grep -Eq "\"failures\": [1-9]" $report_file; then
          # Calling the BF Suggestion server endpoint to start feature extraction.
          payload="{\"task_id\": \"${task_id}\", \"execution\": ${execution}}"
          echo "Sending task info to the BF suggestion service"
          # The --user option is passed through stdin to avoid showing in process list.
          user_option="--user ${bfsuggestion_user}:${bfsuggestion_password}"
          curl --header "Content-Type: application/json" \
               --data "$payload" \
               --max-time 10 \
               --silent \
               --show-error \
               --config - \
               https://bfsuggestion.corp.mongodb.com/tasks <<< $user_option
          echo "Request to BF Suggestion service status: $?"
        fi

  "upload debugsymbols": &upload_debugsymbols
    command: s3.put
    params:
      optional: true
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/mongo-debugsymbols.tgz
      remote_file: ${mongo_debugsymbols}
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}

  "fetch debugsymbols archive": &fetch_debugsymbols_archive
    command: s3.get
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      remote_file: ${mongo_debugsymbols}
      bucket: mciuploads
      local_file: src/mongo-debugsymbols.tgz

  "fetch and build OpenSSL":
    command: shell.exec
    params:
        working_dir: src
        script: |
            set -o errexit
            set -o verbose
            if [ "${build_openssl|}" = "true" ]; then
                bash buildscripts/fetch_and_build_openssl.sh "${python|python3}" "${openssl_make_flags|}" "${openssl_config_flags|}"
            fi

  "use WiredTiger develop":
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose
        if [ "${use_wt_develop|}" = "true" ]; then
        cd src/third_party
        for wtdir in dist examples ext lang src test tools ; do
          rm -rf wiredtiger/$wtdir
          mv wtdevelop/$wtdir wiredtiger/
        done
        fi

  "umount shared scons directory":
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose

        if [ "${scons_cache_scope}" = "shared" ]; then
            if [ "Windows_NT" = "$OS" ]; then
                net use X: /delete || true
            else
                ${set_sudo}
                $sudo umount /efs || true
            fi
        fi

  "build new tools":
    command: shell.exec
    params:
      working_dir: src/src/mongo/gotools/src/github.com/mongodb/mongo-tools
      script: |
        set -o verbose
        set -o errexit

        # make sure newlines in the scripts are handled correctly by windows
        if [ "Windows_NT" = "$OS" ]; then
            set -o igncr
        fi;

        # set_goenv provides set_goenv(), print_ldflags() and print_tags() used below
        . ./set_goenv.sh
        GOROOT="" set_goenv || exit
        env | grep ^GO
        go version

        build_tools="bsondump mongostat mongofiles mongoexport mongoimport mongorestore mongodump mongotop"

        if [ "${build_mongoreplay}" = "true" ]; then
            build_tools="$build_tools mongoreplay"
        fi

        for i in $build_tools; do
            go build -ldflags "$(print_ldflags)" ${args} -tags "$(print_tags ${tooltags})" -o "../../../../../../mongo-tools/$i${exe|}" $i/main/$i.go
            "../../../../../../mongo-tools/$i${exe|}" --version
        done

        mkdir -p ../../../../../../mongo-tools/distsrc
        cp THIRD-PARTY-NOTICES ../../../../../../mongo-tools/distsrc/THIRD-PARTY-NOTICES.gotools

  "get all modified patch files":
    command: shell.exec
    params:
      working_dir: src
      shell: bash
      script: |
        set -o verbose
        set -o errexit

        # For patch builds gather the modified patch files.
        if [ "${is_patch}" = "true" ]; then
          # Get list of patched files
          git diff HEAD --name-only >> patch_files.txt
          if [ -d src/mongo/db/modules/enterprise ]; then
            pushd src/mongo/db/modules/enterprise
            # Update the patch_files.txt in the mongo repo.
            git diff HEAD --name-only >> ~1/patch_files.txt
            popd
          fi
        fi

  "determine resmoke jobs": &determine_resmoke_jobs
    command: shell.exec
    params:
      working_dir: src
      shell: bash
      script: |
        set -o verbose
        set -o errexit

        ${activate_virtualenv}
        $python buildscripts/evergreen_resmoke_job_count.py \
          --taskName ${task_name}                           \
          --buildVariant ${build_variant}                   \
          --jobFactor ${resmoke_jobs_factor|1}              \
          --jobsMax ${resmoke_jobs_max|0}                   \
          --outFile resmoke_jobs_expansion.yml

  "update resmoke jobs expansions": &update_resmoke_jobs_expansions
    command: expansions.update
    params:
      ignore_missing_file: true
      file: src/resmoke_jobs_expansion.yml

  "determine task timeout": &determine_task_timeout
    command: shell.exec
    params:
      working_dir: src
      shell: bash
      script: |
        set -o verbose
        set -o errexit

        ${activate_virtualenv}
        $python buildscripts/evergreen_task_timeout.py \
          --task-name ${task_name}                     \
          --build-variant ${build_variant}             \
          --timeout ${timeout_secs|0}                  \
          --out-file task_timeout_expansions.yml

  "update task timeout expansions": &update_task_timeout_expansions
    command: expansions.update
    params:
      ignore_missing_file: true
      file: src/task_timeout_expansions.yml

  "update task timeout": &update_task_timeout
    command: timeout.update
    params:
      timeout_secs: ${timeout_secs}

  "update bypass expansions": &update_bypass_expansions
    command: expansions.update
    params:
      ignore_missing_file: true
      file: src/bypass_compile_expansions.yml

  "bypass compile and fetch binaries":
    command: shell.exec
    params:
      continue_on_err: true
      working_dir: src
      shell: bash
      script: |
        set -o verbose
        set -o errexit

        if [ -n "${burn_in_bypass}" ]; then
          ${activate_virtualenv}
          # Evergreen executable is in $HOME, so add that to the path.
          PATH=$PATH:$HOME $python buildscripts/burn_in_tags_bypass_compile_and_fetch_binaries.py            \
            --project ${project}                                               \
            --build-variant ${burn_in_bypass}                                  \
            --revision ${revision}                                             \
            --out-file bypass_compile_expansions.yml                           \
            --version-id ${version_id}                                         \
            --json-artifact artifacts.json
        fi

        # For patch builds determine if we can bypass compile.
        if [[ "${is_patch}" = "true" && "${task_name}" = "compile" ]]; then
          ${activate_virtualenv}
          # Evergreen executable is in $HOME, so add that to the path.
          PATH=$PATH:$HOME $python buildscripts/bypass_compile_and_fetch_binaries.py            \
            --project ${project}                                               \
            --build-variant ${build_variant}                                   \
            --revision ${revision}                                             \
            --patch-file patch_files.txt                                       \
            --out-file bypass_compile_expansions.yml                           \
            --json-artifact artifacts.json
        fi

  ### Set expansion macros used in each task.
  ### Note that this function will override task expansions set in update_bypass_expansions
  "set task expansion macros": &set_task_expansion_macros
    command: expansions.update
    params:
      updates:
      - key: activate_virtualenv
        value: |
          # check if virtualenv is set up
          if [ -d "${workdir}/venv" ]; then
            if [ "Windows_NT" = "$OS" ]; then
              # Need to quote the path on Windows to preserve the separator.
              . "${workdir}/venv/Scripts/activate" 2> /tmp/activate_error.log
            else
              . ${workdir}/venv/bin/activate 2> /tmp/activate_error.log
            fi
            if [ $? -ne 0 ]; then
              echo "Failed to activate virtualenv: $(cat /tmp/activate_error.log)"
            fi
            python=python
          else
            python=${python|/opt/mongodbtoolchain/v3/bin/python3}
          fi

          if [ "Windows_NT" = "$OS" ]; then
            export PYTHONPATH="$PYTHONPATH;$(cygpath -w ${workdir}/src)"
          else
            export PYTHONPATH="$PYTHONPATH:${workdir}/src"
          fi

          echo "python set to $(which $python)"
      - key: add_nodejs_to_path
        value: |
          # Add node and npm binaries to PATH
          if [ "Windows_NT" = "$OS" ]; then
            # An "npm" directory might not have been created in %APPDATA% by the Windows installer.
            # Work around the issue by specifying a different %APPDATA% path.
            # See: https://github.com/nodejs/node-v0.x-archive/issues/8141
            export APPDATA=${workdir}/npm-app-data
            export PATH="$PATH:/cygdrive/c/Program Files (x86)/nodejs" # Windows location
            # TODO: this is to work around BUILD-8652
            cd "$(pwd -P | sed 's,cygdrive/c/,cygdrive/z/,')"
          else
            export PATH="$PATH:/opt/node/bin"
          fi
      - key: posix_workdir
        value: eval 'if [ "Windows_NT" = "$OS" ]; then echo $(cygpath -u "${workdir}"); else echo ${workdir}; fi'
      # For ssh disable the options GSSAPIAuthentication, CheckHostIP, StrictHostKeyChecking
      # & UserKnownHostsFile, since these are local connections from one AWS instance to another.
      - key: ssh_connection_options
        value: -o GSSAPIAuthentication=no -o CheckHostIP=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=30 -o ConnectionAttempts=20
      - key: ssh_retries
        value: "10"
      - key: set_sudo
        value: |
          set -o > /tmp/settings.log
          set +o errexit
          grep errexit /tmp/settings.log | grep on
          errexit_on=$?
          # Set errexit "off".
          set +o errexit
          sudo=
          # Use sudo, if it is supported.
          sudo date > /dev/null 2>&1
          if [ $? -eq 0 ]; then
            sudo=sudo
          fi
          # Set errexit "on", if previously enabled.
          if [ $errexit_on -eq 0 ]; then
            set -o errexit
          fi
      - key: mongo_binaries
        value: ${project}/${build_variant}/${revision}/binaries/mongo-${build_id}.${ext|tgz}
      - key: mongo_cryptd
        value: ${project}/${build_variant}/${revision}/binaries/mongo-cryptd-${build_id}.${ext|tgz}
      - key: mh_archive
        value: ${project}/${build_variant}/${revision}/binaries/mh-${build_id}.${ext|tgz}
      - key: mh_debugsymbols
        value: ${project}/${build_variant}/${revision}/debugsymbols/mh-debugsymbols-${build_id}.${ext|tgz}
      - key: mongo_debugsymbols
        value: ${project}/${build_variant}/${revision}/debugsymbols/debugsymbols-${build_id}.${ext|tgz}
      - key: mongo_shell
        value: ${project}/${build_variant}/${revision}/binaries/mongo-shell-${build_id}.${ext|tgz}
      - key: skip_tests
        value: skip_test-${build_id}

  "set up virtualenv": &set_up_virtualenv
    command: shell.exec
    params:
        shell: bash
        script: |
          # exit immediately if virtualenv is not found
          set -o errexit

          virtualenv_loc=$(which ${virtualenv|virtualenv})

          python_loc=$(which ${python|/opt/mongodbtoolchain/v3/bin/python3})
          venv_dir="${workdir}/venv"
          if command -V cygpath; then
            # Sad note: We have to use the Windows path instead of the posix path here.
            # Otherwise, virtualenv may mistakenly resolve paths relative to c:\cygdrive.
            # Creating a virtualenv in cygwin with 'python -m venv', will not work correctly
            # since the paths will be wrong.  We need to use virtualenv and specify the
            # python path as 'C:\python\python_version'
            python_loc=$(cygpath -w $python_loc)
            venv_dir="$(cygpath -w "$venv_dir")"
            "$virtualenv_loc" --python "$python_loc" --system-site-packages "$venv_dir"
          else
            "$python_loc" -m venv --system-site-packages "$venv_dir"
          fi

          export VIRTUAL_ENV_DISABLE_PROMPT=yes

          # Not all git get project calls clone into ${workdir}/src so we allow
          # callers to tell us where the pip requirements files are.
          pip_dir="${pip_dir}"
          if [[ -z $pip_dir ]]; then
            # Default to most common location
            pip_dir="${workdir}/src/etc/pip"
          fi

          # Same as above we have to use quotes to preserve the
          # Windows path separator
          toolchain_txt="$pip_dir/toolchain-requirements.txt"
          ${activate_virtualenv}
          python -m pip install -r "$toolchain_txt" -q
          python -m pip freeze > pip-requirements.txt

  "upload pip requirements": &upload_pip_requirements
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: pip-requirements.txt
      remote_file: ${project}/${build_variant}/${revision}/pip-requirements-${task_id}-${execution}.txt
      bucket: mciuploads
      permissions: public-read
      content_type: atext-plain
      display_name: Pip Requirements

  "do setup":
  - *fetch_artifacts
  - *update_bypass_expansions
  - *fetch_binaries
  - *fetch_debugsymbols_archive
  - *extract_binaries
  - *set_task_expansion_macros
  - *set_up_virtualenv
  - *upload_pip_requirements
  - *check_binary_version
  - *get_buildnumber
  - *set_up_credentials
  - *run_diskstats
  - *monitor_process_threads
  - *collect_system_resource_info

  "do multiversion setup": &do_multiversion_setup
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose

        ${activate_virtualenv}

        rm -rf /data/install /data/multiversion

        edition="${multiversion_edition|base}"
        platform="${multiversion_platform|linux}"
        architecture="${multiversion_architecture|x86_64}"

        $python buildscripts/setup_multiversion_mongodb.py   \
          --installDir /data/install                         \
          --linkDir /data/multiversion                       \
          --edition $edition                                 \
          --platform $platform                               \
          --architecture $architecture                       \
          --useLatest 3.2 3.4 3.6 4.0

        # The platform and architecture for how some of the binaries are reported in
        # https://downloads.mongodb.org/full.json changed between MongoDB 4.0 and MongoDB 4.2.
        # Certain build variants define additional multiversion_*_42_or_later expansions in order to
        # be able to fetch a complete set of versions.

        if [ ! -z "${multiversion_edition_42_or_later}" ]; then
          edition="${multiversion_edition_42_or_later}"
        fi

        if [ ! -z "${multiversion_platform_42_or_later}" ]; then
          platform="${multiversion_platform_42_or_later}"
        fi

        if [ ! -z "${multiversion_architecture_42_or_later}" ]; then
          architecture="${multiversion_architecture_42_or_later}"
        fi

        $python buildscripts/setup_multiversion_mongodb.py   \
          --installDir /data/install                         \
          --linkDir /data/multiversion                       \
          --edition $edition                                 \
          --platform $platform                               \
          --architecture $architecture                       \
          --useLatest 4.2 4.2.1

  "execute resmoke tests": &execute_resmoke_tests
    command: shell.exec
    type: test
    params:
      working_dir: src
      shell: bash
      script: |
        set -o errexit
        set -o verbose

        if [[ ${disable_unit_tests|false} = "false" && ! -f ${skip_tests|/dev/null} ]]; then

        # activate the virtualenv if it has been set up
        ${activate_virtualenv}

        # Set the TMPDIR environment variable to be a directory in the task's working
        # directory so that temporary files created by processes spawned by resmoke.py get
        # cleaned up after the task completes. This also ensures the spawned processes
        # aren't impacted by limited space in the mount point for the /tmp directory.
        export TMPDIR="${workdir}/tmp"
        mkdir -p $TMPDIR

        if [ -f /proc/self/coredump_filter ]; then
          # Set the shell process (and its children processes) to dump ELF headers (bit 4),
          # anonymous shared mappings (bit 1), and anonymous private mappings (bit 0).
          echo 0x13 > /proc/self/coredump_filter

          if [ -f /sbin/sysctl ]; then
            # Check that the core pattern is set explicitly on our distro image instead
            # of being the OS's default value. This ensures that coredump names are consistent
            # across distros and can be picked up by Evergreen.
            core_pattern=$(/sbin/sysctl -n "kernel.core_pattern")
            if [ "$core_pattern" = "dump_%e.%p.core" ]; then
              echo "Enabling coredumps"
              ulimit -c unlimited
            fi
          fi
        fi

        if [ $(uname -s) == "Darwin" ]; then
            core_pattern_mac=$(/usr/sbin/sysctl -n "kern.corefile")
            if [ "$core_pattern_mac" = "dump_%N.%P.core" ]; then
              echo "Enabling coredumps"
              ulimit -c unlimited
            fi
        fi

        extra_args="$extra_args --jobs=${resmoke_jobs|1}"

        if [ ${should_shuffle|true} = true ]; then
          extra_args="$extra_args --shuffle"
        fi

        if [ ${continue_on_failure|true} = true ]; then
          extra_args="$extra_args --continueOnFailure"
        fi

        # We reduce the storage engine's cache size to reduce the likelihood of a mongod process
        # being killed by the OOM killer. The --storageEngineCacheSizeGB command line option is only
        # filled in with a default value here if one hasn't already been specified in the task's
        # definition or build variant's definition.
        set +o errexit
        echo "${resmoke_args} ${test_flags}" | grep -q storageEngineCacheSizeGB
        if [ $? -eq 1 ]; then
          echo "${resmoke_args} ${test_flags}" | grep -q "\-\-storageEngine=inMemory"
          if [ $? -eq 0 ]; then
            # We use a default of 4GB for the InMemory storage engine.
            extra_args="$extra_args --storageEngineCacheSizeGB=4"
          else
            # We use a default of 1GB for all other storage engines.
            extra_args="$extra_args --storageEngineCacheSizeGB=1"
          fi
        fi
        set -o errexit


        # Reduce the JSHeapLimit for the serial_run task task on Code Coverage builder variant.
        if [[ "${build_variant}" = "enterprise-rhel-62-64-bit-coverage" && "${task_name}" = "serial_run" ]]; then
          extra_args="$extra_args --mongodSetParameter {'jsHeapLimitMB':10}"
        fi

        path_value="$PATH"
        if [ ${variant_path_suffix} ]; then
          path_value="$path_value:${variant_path_suffix}"
        fi
        if [ ${task_path_suffix} ]; then
          path_value="$path_value:${task_path_suffix}"
        fi

        # The "resmoke_wrapper" expansion is used by the 'burn_in_tests' task to wrap the resmoke.py
        # invocation. It doesn't set any environment variables and should therefore come last in
        # this list of expansions.
        set +o errexit
        PATH="$path_value"                              \
            AWS_PROFILE=${aws_profile_remote}           \
            ${gcov_environment}                         \
            ${lang_environment}                         \
            ${san_options}                              \
            ${san_symbolizer}                           \
            ${snmp_config_path}                         \
            ${resmoke_wrapper}                          \
            $python buildscripts/evergreen_run_tests.py \
                ${resmoke_args}                         \
                $extra_args                             \
                ${test_flags}                           \
                --log=buildlogger                       \
                --staggerJobs=on                        \
                --buildId=${build_id}                   \
                --distroId=${distro_id}                 \
                --executionNumber=${execution}          \
                --projectName=${project}                \
                --gitRevision=${revision}               \
                --revisionOrderId=${revision_order_id}  \
                --taskId=${task_id}                     \
                --taskName=${task_name}                 \
                --variantName=${build_variant}          \
                --versionId=${version_id}               \
                --archiveFile=archive.json              \
                --reportFile=report.json                \
                --perfReportFile=perf.json
        resmoke_exit_code=$?
        set -o errexit

        # 74 is exit code for IOError on POSIX systems, which is raised when the machine is
        # shutting down.
        #
        # 75 is exit code resmoke.py uses when the log output would be incomplete due to failing
        # to communicate with logkeeper.
        if [[ $resmoke_exit_code = 74 || $resmoke_exit_code = 75 ]]; then
          echo $resmoke_exit_code > run_tests_infrastructure_failure
          exit 0
        elif [ $resmoke_exit_code != 0 ]; then
          # On failure save the resmoke exit code.
          echo $resmoke_exit_code > resmoke_error_code
        fi
        exit $resmoke_exit_code
        fi # end if [[ ${disable_unit_tests} && ! -f ${skip_tests|/dev/null} ]]

  "retrieve generated test configuration": &retrieve_generated_test_configuration
    command: s3.get
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      bucket: mciuploads
      remote_file: ${project}/${build_variant}/${revision}/generate_tasks/${task}_gen-${build_id}.tgz
      local_file: "generate_tasks_config.tgz"

  "extract generated test configuration": &extract_generated_test_configuration
    command: shell.exec
    type: test
    params:
      shell: bash
      script: |
        set -o verbose
        set -o errexit

        target_dir="src/generated_resmoke_config"
        mkdir -p $target_dir
        mv generate_tasks_config.tgz $target_dir

        cd $target_dir
        tar xzf generate_tasks_config.tgz

  "generate burn in tags":
    - command: expansions.write
      params:
        file: src/expansions.yml
    - *configure_evergreen_api_credentials
    - command: shell.exec
      type: test
      params:
        working_dir: src
        shell: bash
        script: |
          set -o errexit

          ${activate_virtualenv}
          PATH=$PATH:$HOME $python buildscripts/burn_in_tags.py --expansion-file expansions.yml
    - command: archive.targz_pack
      params:
        target: burn_in_tags_gen.tgz
        source_dir: src/generated_burn_in_tags_config
        include:
          - "*"
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: burn_in_tags_gen.tgz
        remote_file: ${project}/${build_variant}/${revision}/burn_in_tags_gen/burn_in_tags_gen-${build_id}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Burn_in_tags Task Config - Execution ${execution}
    - command: generate.tasks
      params:
        files:
          - src/generated_burn_in_tags_config/burn_in_tags_gen.json

  "generate randomized multiversion tasks":
    - command: manifest.load
    - *git_get_project
    - *set_task_expansion_macros
    - *set_up_virtualenv
    - *upload_pip_requirements
    - *configure_evergreen_api_credentials
    - command: expansions.write
      params:
        file: src/expansions.yml

    - command: shell.exec
      type: test
      params:
        working_dir: src
        script: |
          set -o errexit

          ${activate_virtualenv}
          $python buildscripts/evergreen_generate_resmoke_tasks.py --expansion-file expansions.yml --verbose
    - *do_multiversion_setup
    - command: shell.exec
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          ${activate_virtualenv}
          $python buildscripts/evergreen_gen_multiversion_tests.py generate-exclude-files --suite=${suite} --task-path-suffix=${use_multiversion} --is-generated-suite=true
    - command: archive.targz_pack
      params:
        target: generate_tasks_config.tgz
        source_dir: src/generated_resmoke_config
        include:
          - "*"

    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: generate_tasks_config.tgz
        remote_file: ${project}/${build_variant}/${revision}/generate_tasks/${task_name}-${build_id}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Generated Task Config - Execution ${execution}
        optional: true

    - command: generate.tasks
      params:
        optional: true
        files:
          - src/generated_resmoke_config/*.json

  "generate resmoke tasks":
    - command: manifest.load
    - *git_get_project
    - *set_task_expansion_macros
    - *set_up_virtualenv
    - *upload_pip_requirements
    - *configure_evergreen_api_credentials
    - command: expansions.write
      params:
        file: src/expansions.yml

    - command: shell.exec
      type: test
      params:
        working_dir: src
        script: |
          set -o errexit

          ${activate_virtualenv}
          $python buildscripts/evergreen_generate_resmoke_tasks.py --expansion-file expansions.yml --verbose

    - command: archive.targz_pack
      params:
        target: generate_tasks_config.tgz
        source_dir: src/generated_resmoke_config
        include:
          - "*"

    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: generate_tasks_config.tgz
        remote_file: ${project}/${build_variant}/${revision}/generate_tasks/${task_name}-${build_id}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Generated Task Config - Execution ${execution}
        optional: true

    - command: generate.tasks
      params:
        optional: true
        files:
          - src/generated_resmoke_config/*.json

  "run generated tests":
  - *retrieve_generated_test_configuration
  - *extract_generated_test_configuration
  - command: expansions.update
    params:
      updates:
      - key: aws_key_remote
        value: ${mongodatafiles_aws_key}
      - key: aws_profile_remote
        value: mongodata_aws
      - key: aws_secret_remote
        value: ${mongodatafiles_aws_secret}
  - *set_up_remote_credentials
  - *determine_resmoke_jobs
  - *update_resmoke_jobs_expansions
  - *execute_resmoke_tests
    # The existence of the "run_tests_infrastructure_failure" file indicates this failure isn't
    # directly actionable. We use type=setup rather than type=system or type=test for this command
    # because we don't intend for any human to look at this failure.
  - command: shell.exec
    type: setup
    params:
      working_dir: src
      script: |
        set -o verbose
        if [ -f run_tests_infrastructure_failure ]; then
          exit $(cat run_tests_infrastructure_failure)
        fi

  "run tests":
    - *determine_task_timeout
    - *update_task_timeout_expansions
    - *update_task_timeout
    - command: expansions.update
      params:
        updates:
        - key: aws_key_remote
          value: ${mongodatafiles_aws_key}
        - key: aws_profile_remote
          value: mongodata_aws
        - key: aws_secret_remote
          value: ${mongodatafiles_aws_secret}
    - *set_up_remote_credentials
    - *determine_resmoke_jobs
    - *update_resmoke_jobs_expansions
    - *execute_resmoke_tests
      # The existence of the "run_tests_infrastructure_failure" file indicates this failure isn't
      # directly actionable. We use type=setup rather than type=system or type=test for this command
      # because we don't intend for any human to look at this failure.
    - command: shell.exec
      type: setup
      params:
        working_dir: src
        script: |
          set -o verbose
          if [ -f run_tests_infrastructure_failure ]; then
            exit $(cat run_tests_infrastructure_failure)
          fi

  "scons lint":
    - command: shell.exec
      type: test
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          ${activate_virtualenv}
          export MYPY="$(
            if command -V cygpath 2>/dev/null; then
                PATH+=":$(cypath "${workdir}")/venv_3/Scripts"
            else
                PATH+=":${workdir}/venv_3/bin"
            fi
            PATH+=':/opt/mongodbtoolchain/v3/bin'
            which mypy
          )"
          echo "Found mypy executable at '$MYPY'"
          export extra_flags=""
          if [[ ${is_patch|false} == "true" ]]; then
            extra_flags="--lint-scope=changed"
          fi

          ${compile_env|} python3 ./buildscripts/scons.py ${compile_flags|} $extra_flags --stack-size=1024 GITDIFFFLAGS="--cached" ${targets}

  "scons compile":
    command: shell.exec
    type: test
    params:
      working_dir: src
      shell: bash
      script: |
        set -o errexit
        set -o verbose

        if [ "${is_patch}" = "true" ] && [ "${bypass_compile|false}" = "true" ]; then
          exit 0
        fi
        rm -rf ${install_directory|/data/mongo-install-directory}

        extra_args=""
        if [ -n "${num_scons_link_jobs_available|}" ]; then
          echo "Changing SCons to run with --jlink=${num_scons_link_jobs_available|}"
          extra_args="$extra_args --jlink=${num_scons_link_jobs_available|}"
        fi

        if [ "${scons_cache_scope|}" = "shared" ]; then
          extra_args="$extra_args --cache-debug=scons_cache.log"
        fi

        # Enable performance debugging
        extra_args="$extra_args --debug=time"

        # If we are doing a patch build or we are building a non-push
        # build on the waterfall, then we don't need the --release
        # flag. Otherwise, this is potentially a build that "leaves
        # the building", so we do want that flag. The non --release
        # case should auto enale the faster decider when
        # applicable. Furthermore, for the non --release cases we can
        # accelerate the build slightly for situations where we invoke
        # SCons multiple times on the same machine by allowing SCons
        # to assume that implicit dependencies are cacheable across
        # runs.
        if [ "${is_patch|false}" = "true" ] || [ -z "${push_bucket|}" ] || [ "${compiling_for_test|false}" = "true" ]; then
          extra_args="$extra_args --implicit-cache --build-fast-and-loose=on"
        else
          extra_args="$extra_args --release"
        fi

        ${activate_virtualenv}

        ${compile_env|} $python ./buildscripts/scons.py                                     \
            ${compile_flags|} ${task_compile_flags|} ${task_compile_flags_extra|}           \
            ${scons_cache_args|} $extra_args                                                \
            ${targets} ${additional_targets|} MONGO_VERSION=${version} || exit_status=$?
        # If compile fails we do not run any tests
        if [[ $exit_status -ne 0 ]]; then
          if [[ "${dump_scons_config_on_failure}" == true ]]; then
            echo "Dumping build/scons/config.log"
            cat build/scons/config.log
          fi
          touch ${skip_tests}
        fi
        exit $exit_status

  "generate compile expansions":
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose

        # We get the raw version string (r1.2.3-45-gabcdef) from git
        MONGO_VERSION=$(git describe --abbrev=7)
        # If this is a patch build, we add the patch version id to the version string so we know
        # this build was a patch, and which evergreen task it came from
        if [ "${is_patch}" = "true" ] && [ "${bypass_compile|false}" = "false" ]; then
          MONGO_VERSION="$MONGO_VERSION-patch-${version_id}"
        fi

        ${activate_virtualenv}

        # shared scons cache testing
        # if 'scons_cache_scope' enabled and project level 'disable_shared_scons_cache' is not true
        # 'scons_cache_scope' is set on a per variant basis
        # 'disable_shared_scons_cache' is set on a project level and applies to all variants

        # Shared - if scons_cache_scope is set, then use new shared scons cache settings
        if [ ! -z ${scons_cache_scope} ]; then

          if [ "${disable_shared_scons_cache}" = "true" ]; then

            echo "SCons Cache disabled. All shared scons settings will be ignored"
            scons_cache_scope=none

          else
            scons_cache_scope=${scons_cache_scope}
          fi

          if [ "$scons_cache_scope" = "shared" ]; then
            set +o errexit
            if [ "Windows_NT" = "$OS" ]; then
              ./win_mount.sh
            else
              mount | grep "\/efs" > /dev/null
              if [ $? -eq 0 ]; then
                echo "Shared cache is already mounted"
              else
                echo "Shared cache - mounting file system"
                ${set_sudo}
                $sudo mount /efs
              fi
            fi
            set -o errexit
          fi

          echo "Shared Cache with setting: ${scons_cache_scope}"
          MONGO_VERSION=$MONGO_VERSION SCONS_CACHE_MODE=${scons_cache_mode|nolinked} SCONS_CACHE_SCOPE=$scons_cache_scope IS_PATCH=${is_patch} IS_COMMIT_QUEUE=${is_commit_queue|false} $python buildscripts/generate_compile_expansions_shared_cache.py --out compile_expansions.yml

        # Legacy Expansion generation
        else
          echo "Using legacy expansion generation"
          # Proceed with regular expansions generated
          # This script converts the generated version string into a sanitized version string for
          # use by scons and uploading artifacts as well as information about for the scons cache.
          MONGO_VERSION=$MONGO_VERSION SCONS_CACHE_MODE=${scons_cache_mode|nolinked} USE_SCONS_CACHE=${use_scons_cache|false} $python buildscripts/generate_compile_expansions.py --out compile_expansions.yml
        fi

  "apply compile expansions":
    command: expansions.update
    params:
      file: src/compile_expansions.yml

  "load aws test credentials":
    - command: shell.exec
      params:
        silent: true
        working_dir: src
        script: |
          set -o errexit
          echo "const AWS_KMS_SECRET_ID = '${aws_kms_access_key_id}';" >> src/mongo/db/modules/enterprise/jstests/fle/lib/aws_secrets.js
          echo "const AWS_KMS_SECRET_KEY = '${aws_kms_secret_access_key}';" >> src/mongo/db/modules/enterprise/jstests/fle/lib/aws_secrets.js

  "generate multiversion tasks":
    - command: manifest.load
    - *git_get_project
    - *set_task_expansion_macros
    - *set_up_virtualenv
    - *configure_evergreen_api_credentials
    - command: expansions.write
      params:
        file: src/expansions.yml
    - *do_multiversion_setup
    - command: shell.exec
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          ${activate_virtualenv}
          $python buildscripts/evergreen_gen_multiversion_tests.py run --expansion-file expansions.yml

    - command: shell.exec
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          ${activate_virtualenv}
          $python buildscripts/evergreen_gen_multiversion_tests.py generate-exclude-files --suite=${task_name} --task-path-suffix=${task_path_suffix} --is-generated-suite=true

    - command: archive.targz_pack
      params:
        target: generate_tasks_config.tgz
        source_dir: src/generated_resmoke_config
        include:
          - "*"

    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: generate_tasks_config.tgz
        remote_file: ${project}/${build_variant}/${revision}/generate_tasks/${task_name}-${build_id}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Generated Task Config - Execution ${execution}

    - command: generate.tasks
      params:
        files:
          - src/generated_resmoke_config/*.json

  "generate fuzzer tasks":
    - command: manifest.load
    - *git_get_project
    - *set_task_expansion_macros
    - *set_up_virtualenv
    - *upload_pip_requirements

    - command: expansions.write
      params:
        file: src/expansions.yml

    - command: shell.exec
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          ${activate_virtualenv}
          $python buildscripts/evergreen_gen_fuzzer_tests.py --expansion-file expansions.yml

    - command: archive.targz_pack
      params:
        target: generate_tasks_config.tgz
        source_dir: src/generated_resmoke_config
        include:
          - "*"

    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: generate_tasks_config.tgz
        remote_file: ${project}/${build_variant}/${revision}/generate_tasks/${name}_gen-${build_id}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Generated Task Config - Execution ${execution}

    - command: generate.tasks
      params:
        files:
          - src/generated_resmoke_config/${name}.json

  "setup jstestfuzz":
    - command: shell.exec
      params:
        working_dir: src
        shell: bash
        script: |
          set -o errexit
          set -o verbose

          ${add_nodejs_to_path}

          git clone git@github.com:10gen/jstestfuzz.git

          pushd jstestfuzz
            npm install
            npm run prepare
          popd

  "lint fuzzer sanity patch":
    - command: shell.exec
      type: test
      params:
        working_dir: src
        shell: bash
        script: |
          set -eo pipefail
          set -o verbose

          ${add_nodejs_to_path}

          # Run parse-jsfiles on 50 files at a time with 32 processes in parallel.
          # Grep returns 1 if it fails to find a match.
          (grep "\.js$" modified_and_created_patch_files.txt || true) | xargs -P 32 -L 50 npm run --prefix jstestfuzz parse-jsfiles --

  "lint fuzzer sanity all":
    - command: shell.exec
      type: test
      params:
        working_dir: src
        shell: bash
        script: |
          set -eo pipefail
          set -o verbose

          ${add_nodejs_to_path}

          # Run parse-jsfiles on 50 files at a time with 32 processes in parallel.
          find "$PWD/jstests" "$PWD/src/mongo/db/modules/enterprise" -name "*.js" -print | xargs -P 32 -L 50 npm run --prefix jstestfuzz parse-jsfiles --

  "run jstestfuzz":
    - command: shell.exec
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          cp mongodb*/bin/mongod .

          git clone --depth 1 git@github.com:10gen/mongo-enterprise-modules.git jstests/enterprise_tests
          git clone --depth 1 git@github.com:10gen/QA.git jstests/qa_tests

    - command: shell.exec
      type: test
      params:
        working_dir: src/jstestfuzz
        script: |
          set -o errexit
          set -o verbose

          ${add_nodejs_to_path}

          npm run ${npm_command|jstestfuzz} -- ${jstestfuzz_vars} --branch ${branch_name}

    - command: archive.targz_pack
      params:
        target: "jstests.tgz"
        source_dir: "src/jstestfuzz"
        include:
          - "out/*.js"
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: jstests.tgz
        remote_file: ${project}/${build_variant}/${revision}/jstestfuzz/${task_id}-${execution}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Generated Tests - Execution ${execution}

  "run idl tests":
    - command: shell.exec
      type: test
      params:
        working_dir: src
        script: |
          set -o verbose
          set -o errexit

          ${activate_virtualenv}
          $python buildscripts/idl/run_tests.py

  "do snmp setup":
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose

        mkdir -p snmpconf
        cp -f src/mongo/db/modules/enterprise/docs/mongod.conf.master snmpconf/mongod.conf

  "do watchdog setup":
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose

        bash jstests/watchdog/charybdefs_setup.sh

  "cleanup environment":
    command: shell.exec
    params:
      script: |
        set -o verbose

        rm -rf src /data/db/* mongo-diskstats* mongo-*.tgz ~/.aws ~/.boto venv
        exit 0

  "kill processes":
    command: shell.exec
    params:
      silent: true
      script: |
        process_kill_list="(^cl\.exe$|bsondump|java|lein|lldb|mongo|python|_test$|_test\.exe$)"
        # Exclude Evergreen agent processes and other system daemons
        process_exclude_list="(main|tuned|evergreen)"

        if [ "Windows_NT" = "$OS" ]; then
          # Get the list of Windows tasks (tasklist list format):
          # - Transpose the Image Name and PID
          # - The first column has the process ID
          # - The second column (and beyond) has task name
          # - Grep for the task names of interest while ignoring any names that are in the exclude list

          processes=$(tasklist /fo:csv | awk -F'","' '{x=$1; gsub("\"","",x); print $2, x}' | grep -iE "$process_kill_list" | grep -ivE "$process_exclude_list")

          # Kill the Windows process by process ID with force (/f)
          kill_process () { pid=$(echo $1 | cut -f1 -d ' '); echo "Killing process $1"; taskkill /pid "$pid" /f; }
        else
          # Get the list of Unix tasks (pgrep full & long):
          # - Grep for the task names of interest while ignoring any names that are in the exclude list
          # - The first column has the process ID
          # - The second column (and beyond) has task name

          # There are 2 "styles" of pgrep, figure out which one works.
          # Due to https://bugs.launchpad.net/ubuntu/+source/procps/+bug/1501916
          # we cannot rely on the return status ($?) to detect if the option is supported.
          pgrep -f --list-full ".*" 2>&1 | grep -qE "(illegal|invalid|unrecognized) option"
          if [ $? -ne 0 ]; then
            pgrep_list=$(pgrep -f --list-full "$process_kill_list")
          else
            pgrep_list=$(pgrep -f -l "$process_kill_list")
          fi

          # Since a process name might have a CR or LF in it, we need to delete any lines from
          # pgrep which do not start with space(s) and 1 digit and trim any leading spaces.
          processes=$(echo "$pgrep_list" | grep -ivE "$process_exclude_list" | sed -e '/^ *[0-9]/!d; s/^ *//; s/[[:cntrl:]]//g;')

          # Kill the Unix process ID with signal KILL (9)
          kill_process () { pid=$(echo $1 | cut -f1 -d ' '); echo "Killing process $1"; kill -9 $pid; }
        fi
        # Since a full process name can have spaces, the IFS (internal field separator)
        # should not include a space, just a LF & CR
        IFS=$(printf "\n\r")
        for process in $processes
        do
          kill_process "$process"
        done

        exit 0

  "run kitchen":
    command: shell.exec
    type: test
    params:
      shell: bash
      working_dir: src/buildscripts/package_test
      script: |
        set -o errexit

        export KITCHEN_ARTIFACTS_URL="https://s3.amazonaws.com/mciuploads/${project}/${build_variant}/${revision}/artifacts/${build_id}-packages.tgz"
        export KITCHEN_SECURITY_GROUP="${kitchen_security_group}"
        export KITCHEN_SSH_KEY_ID="${kitchen_ssh_key_id}"
        export KITCHEN_SUBNET="${kitchen_subnet}"
        export KITCHEN_VPC="${kitchen_vpc}"

        ${activate_virtualenv}
        # set expiration tag 2 hours in the future, since no test should take this long
        export KITCHEN_EXPIRE="$($python -c 'import datetime; print((datetime.datetime.utcnow() + datetime.timedelta(hours=2)).strftime("%Y-%m-%d %H:%M:%S"))')"

        for i in {1..3}
        do
          # kitchen commands use regex so 'kitchen verify amazon' matches both amazon and amazon2
          # that's why we pass $ at the end of "${packager_distro}"
          if ! kitchen verify "${packager_distro}"\$; then
            verified="false"
            kitchen destroy "${packager_distro}"\$ || true
            sleep 30
          else
            verified="true"
            break
          fi
        done

        kitchen destroy "${packager_distro}"\$ || true
        test "$verified" = "true"

  "copy ec2 monitor files": &copy_ec2_monitor_files
    command: shell.exec
    params:
      background: true
      system_log: true
      working_dir: src
      silent: false
      script: |
        while [ 1 ]
        do
          # Tar/zip monitor files on remote host.
          if [ -z "${ec2_monitor_files}" ] || [ -z "${instance_id}" ]; then
            exit 0
          fi
          # Ensure we use the latest private_ip_address, as it could change if the EC2 instance
          # has been stopped and started.
          ${activate_virtualenv}
          # Specify '--mode start' to ensure the remote instance is running.
          monitor_ec2_yml=monitor_ec2.yml
          $python buildscripts/aws_ec2.py --imageId ${instance_id} --mode start --yamlFile $monitor_ec2_yml
          echo "AMI EC2 instance ${instance_id} status: $(cat $monitor_ec2_yml)"
          private_ip_address=$($python buildscripts/yaml_key_value.py --yamlFile $monitor_ec2_yml --yamlKey private_ip_address)
          if [ -z "$private_ip_address" ]; then
            echo "Cannot determine the IP address for the remote monitor."
            continue
          fi
          cmd="${tar|tar} czf ec2_monitor_files.tgz ${ec2_monitor_files}"
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ssh_connection_options="$ssh_connection_options -o ConnectionAttempts=3"
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@$private_ip_address             \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --commands "$cmd"
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@$private_ip_address             \
            --operation "copy_from"                          \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --file ec2_monitor_files.tgz
          sleep 30
        done

  "set up EC2 instance": &set_up_ec2_instance
    - command: shell.exec
      params:
        working_dir: src
        script: |

          set -o errexit
          ${activate_virtualenv}

          if [ ! -z "${subnet_id}" ]; then
            subnet_id="-n ${subnet_id}"
          fi

          for security_group_id in ${security_group_ids}
          do
            security_group_ids="$security_group_ids -g $security_group_id"
          done

          if [ -z "${security_group_ids}" ]; then
            for security_group in ${security_groups}
            do
              security_groups="$security_groups -s $security_group"
            done
          fi

          if [ -n "${ec2_expire_hours}" ]; then
            expire_hours="-e ${ec2_expire_hours}"
            # Since Windows hosts are expensive to keep running we'll expire it after 3 hours.
            if [ "Windows_NT" = "$OS" ]; then
              expire_hours="-e 3"
            fi
          fi

          # Clone another instance of this host in EC2.
          buildscripts/launch_evergreen_ec2_instance.sh \
            $expire_hours                               \
            -k ${ssh_key_id}                            \
            $security_groups                            \
            $security_group_ids                         \
            $subnet_id                                  \
            -t "AMI Evergreen ${task_id}"               \
            -y ${aws_ec2_yml}

    - command: expansions.update
      params:
        file: src/${aws_ec2_yml}

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          # Copy mount_drives.sh script to remote host.
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --operation "copy_to"                            \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --file buildscripts/mount_drives.sh

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          # Mount /data on the attached drive(s), more than 1 indicates a RAID set.
          ${set_sudo}
          script_opts="-d '${data_device_names}'"
          if [ ! -z "${raid_data_device_name}" ]; then
            script_opts="$script_opts -r ${raid_data_device_name}"
          fi
          if [ ! -z "${fstype}" ]; then
            script_opts="$script_opts -t ${fstype}"
          fi
          if [ ! -z "${fs_options}" ]; then
            script_opts="$script_opts -o '${fs_options}'"
          fi
          # Mount /log on the attached drive.
          if [ ! -z "${log_device_name}" ]; then
            script_opts="$script_opts -l '${log_device_name}'"
            log="/log"
          fi
          group=$(id -Gn $USER | cut -f1 -d ' ') || true
          user_group="$USER:$group"
          script_opts="$script_opts -u $user_group"
          data_db=/data/db
          cmds="$sudo bash mount_drives.sh $script_opts; mount; ls -ld $data_db $log; df"
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --commands "$cmds"

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          # Create remote_dir, if specified as expansion macro and is not '.' (pwd).
          if [[ -z "${remote_dir|}" || ${remote_dir} == "." ]]; then
            exit 0
          fi
          ${set_sudo}
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          group=$(id -Gn $USER | cut -f1 -d ' ') || true
          user_group="$USER:$group"
          set_permission="chmod 777 ${remote_dir}"
          if [ "Windows_NT" = "$OS" ]; then
            set_permission="setfacl -s user::rwx,group::rwx,other::rwx ${remote_dir}"
          fi
          cmds="$sudo mkdir -p ${remote_dir}; $sudo chown $user_group ${remote_dir}; $set_permission; ls -ld ${remote_dir}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --commands "$cmds"

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          # Copy buildscripts, pytests and mongoDB executables to the remote host.
          file_param="--file etc --file buildscripts --file pytests"
          mongo_executables="mongo mongod mongos"
          for executable in $mongo_executables
          do
            file_param="$file_param --file $executable${exe}"
          done
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --operation "copy_to"                            \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            $file_param                                      \
            --remoteDir ${remote_dir}

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          # Set up virtualenv on remote.
          cmds="python_loc=\$(which \${python|/opt/mongodbtoolchain/v3/bin/python3})"
          cmds="$cmds; remote_dir=${remote_dir|.}"
          cmds="$cmds; if [ \"Windows_NT\" = \"$OS\" ]; then python_loc=\$(cygpath -w \$python_loc); remote_dir=\$(cygpath -w \$remote_dir); fi"
          cmds="$cmds; virtualenv --python \$python_loc --system-site-packages ${virtualenv_dir|venv}"
          cmds="$cmds; activate=\$(find ${virtualenv_dir|venv} -name 'activate')"
          cmds="$cmds; . \$activate"
          cmds="$cmds; pip3 install -r \$remote_dir/etc/pip/powercycle-requirements.txt"
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --commands "$cmds"

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          if [ "Windows_NT" = "$OS" ]; then
            exit 0
          fi
          # Enable core dumps on non-Windows remote hosts.
          # The core pattern must specify a director, since mongod --fork will chdir("/")
          # and cannot generate a core dump there (see SERVER-21635).
          # We need to reboot the host for the core limits to take effect.
          ${set_sudo}
          core_pattern=${remote_dir}/dump_%e.%p.core
          sysctl_conf=/etc/sysctl.conf
          cmds="ulimit -a"
          cmds="$cmds; echo \"$USER - core unlimited\" | $sudo tee -a /etc/security/limits.conf"
          cmds="$cmds; if [ -f $sysctl_conf ]"
          cmds="$cmds; then grep ^kernel.core_pattern $sysctl_conf"
          cmds="$cmds;    if [ \$? -eq  0 ]"
          cmds="$cmds;    then $sudo sed -i \"s,kernel.core_pattern=.*,kernel.core_pattern=$core_pattern,\" $sysctl_conf"
          cmds="$cmds;    else echo \"kernel.core_pattern=$core_pattern\" | $sudo tee -a $sysctl_conf"
          cmds="$cmds;    fi"
          cmds="$cmds; else echo Cannot change the core pattern and no core dumps will be generated."
          cmds="$cmds; fi"
          # The following line for restarting the machine is based on
          # https://unix.stackexchange.com/a/349558 in order to ensure the ssh client gets a
          # response from the remote machine before it restarts.
          cmds="$cmds; nohup $sudo reboot &>/dev/null & exit"
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --commands "$cmds"

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          if [ "Windows_NT" = "$OS" ]; then
            exit 0
          fi
          # Always exit successfully, as this is just informational.
          trap 'echo "Trapped exit code $?, exiting with 0"; exit 0' EXIT
          # Print the ulimit & kernel.core_pattern
          cmds="uptime"
          cmds="$cmds; ulimit -a"
          cmds="$cmds; if [ -f /sbin/sysctl ]"
          cmds="$cmds; then /sbin/sysctl kernel.core_pattern"
          cmds="$cmds; fi"
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|3}                       \
            --commands "$cmds"

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          ${set_sudo}
          # Set up curator to collect system & process stats on remote.
          if [ "Windows_NT" = "$OS" ]; then
             variant=windows
          else
             variant=ubuntu1604
          fi
          # Download stable version of curator
          curator_hash=117d1a65256ff78b6d15ab79a1c7088443b936d0
          curator_url="https://s3.amazonaws.com/boxes.10gen.com/build/curator/curator-dist-$variant-$curator_hash.tar.gz"
          cmds="curl -s $curator_url | tar -xzv"
          if [ "Windows_NT" = "$OS" ]; then
            # Since curator runs as SYSTEM user, ensure the output files can be accessed.
            cmds="$cmds; touch ${monitor_system_file}; chmod 777 ${monitor_system_file}"
            cmds="$cmds; cygrunsrv --install curator_sys --path curator --chdir \$HOME --args 'stat system --file ${monitor_system_file}'"
            cmds="$cmds; touch ${monitor_proc_file}; chmod 777 ${monitor_proc_file}"
            cmds="$cmds; cygrunsrv --install curator_proc --path curator --chdir \$HOME --args 'stat process-all --file ${monitor_proc_file}'"
            cmds="$cmds; cygrunsrv --start curator_sys"
            cmds="$cmds; cygrunsrv --start curator_proc"
          else
            cmds="$cmds; cmd=\"@reboot cd \$HOME && $sudo ./curator stat system >> ${monitor_system_file}\""
            cmds="$cmds; (crontab -l ; echo \"\$cmd\") | crontab -"
            cmds="$cmds; cmd=\"@reboot cd \$HOME && $sudo ./curator stat process-all >> ${monitor_proc_file}\""
            cmds="$cmds; (crontab -l ; echo \"\$cmd\") | crontab -"
            cmds="$cmds; crontab -l"
            cmds="$cmds; { $sudo \$HOME/curator stat system --file ${monitor_system_file} > /dev/null 2>&1 & $sudo \$HOME/curator stat process-all --file ${monitor_proc_file} > /dev/null 2>&1 & } & disown"
          fi
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --commands "$cmds"

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          ${set_sudo}
          # Many systems have the firewall disabled, by default. In case the firewall is
          # enabled we add rules for the mongod ports on the remote.
          # RHEL 7 firewall rules
          if [ ! -z "$(which firewall-cmd 2> /dev/null)" ]; then
            cmds="$sudo firewall-cmd --permanent --zone=public --add-port=ssh/tcp"
            cmds="$cmds; $sudo firewall-cmd --permanent --zone=public --add-port=${standard_port}/tcp"
            cmds="$cmds; $sudo firewall-cmd --permanent --zone=public --add-port=${secret_port}/tcp"
            cmds="$cmds; $sudo firewall-cmd --reload"
            cmds="$cmds; $sudo firewall-cmd --list-all"
          # ArchLinux, Debian, RHEL 6 firewall rules
          elif [ ! -z "$($sudo iptables --list 2> /dev/null)" ]; then
            cmds="$sudo iptables -I INPUT 1 -p tcp --dport ssh -j ACCEPT"
            cmds="$cmds; $sudo iptables -I INPUT 1 -p tcp --dport ${standard_port} -j ACCEPT"
            cmds="$cmds; $sudo iptables -I INPUT 1 -p tcp --dport ${secret_port} -j ACCEPT"
            if [ -d /etc/iptables ]; then
              rules_file=/etc/iptables/iptables.rules
            elif [ -f /etc/sysconfig/iptables ]; then
              rules_file=/etc/sysconfig/iptables
            else
              rules_file=/etc/iptables.up.rules
            fi
            cmds="$cmds; $sudo iptables-save | $sudo tee $rules_file"
            cmds="$cmds; $sudo iptables --list-rules"
          elif [ ! -z "$($sudo service iptables status 2> /dev/null)" ]; then
            cmds="$sudo iptables -I INPUT 1 -p tcp --dport ssh -j ACCEPT"
            cmds="$cmds; $sudo iptables -I INPUT 1 -p tcp --dport ${standard_port} -j ACCEPT"
            cmds="$cmds; $sudo iptables -I INPUT 1 -p tcp --dport ${secret_port} -j ACCEPT"
            cmds="$cmds; $sudo service iptables save"
            cmds="$cmds; $sudo service iptables status"
          # Ubuntu firewall rules
          elif [ ! -z "$($sudo ufw status 2> /dev/null)" ]; then
            cmds="$sudo ufw allow ssh/tcp"
            cmds="$cmds; $sudo ufw allow ${standard_port}/tcp"
            cmds="$cmds; $sudo ufw allow ${secret_port}/tcp"
            cmds="$cmds; $sudo ufw reload"
            cmds="$cmds; $sudo ufw status"
          # SuSE firewall rules
          # TODO: Add firewall rules using SuSEfirewall2
          elif [ ! -z "$($sudo /sbin/SuSEfirewall2 help 2> /dev/null)" ]; then
            cmds="$sudo /sbin/SuSEfirewall2 stop"
            cmds="$cmds; $sudo /sbin/SuSEfirewall2 off"
          # Windows firewall rules
          elif [ ! -z "$(netsh advfirewall show store 2> /dev/null)" ]; then
            add_rule="netsh advfirewall firewall add rule"
            cmds="$add_rule name='MongoDB port ${standard_port} in' dir=in action=allow protocol=TCP localport=${standard_port}"
            cmds="$cmds; $add_rule name='MongoDB port ${standard_port} out' dir=in action=allow protocol=TCP localport=${standard_port}"
            cmds="$cmds; $add_rule name='MongoDB port ${secret_port} in' dir=in action=allow protocol=TCP localport=${secret_port}"
            cmds="$cmds; $add_rule name='MongoDB port ${secret_port} out' dir=in action=allow protocol=TCP localport=${secret_port}"
            cmds="$cmds; netsh advfirewall firewall show rule name=all | grep -A 13 'MongoDB'"
          else
            echo "Firewall not active or unknown firewall command on this platform"
            exit 0
          fi
          set -o errexit
          if [ ! -z "$cmds" ]; then
            ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
            ${activate_virtualenv}
            $python buildscripts/remote_operations.py          \
              --verbose                                        \
              --userHost $USER@${private_ip_address}           \
              --sshConnectionOptions "$ssh_connection_options" \
              --retries ${ssh_retries|0}                       \
              --commands "$cmds"
          fi

    - command: shell.exec
      params:
        shell: bash
        working_dir: src
        script: |
          set -o errexit
          if [[ "Windows_NT" != "$OS" || -z "${windows_crash_zip}" ]]; then
            exit 0
          fi
          # Install NotMyFault, used to crash Windows.
          cmds="curl -s -o ${windows_crash_zip} ${windows_crash_dl}"
          cmds="$cmds; unzip -q ${windows_crash_zip} -d ${windows_crash_dir}"
          cmds="$cmds; chmod +x ${windows_crash_dir}/*.exe"
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          ${activate_virtualenv}
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries|0}                       \
            --commands "$cmds"

    - *copy_ec2_monitor_files

  ### Determine & set remote EC2 IP address ###
  "get EC2 address": &get_ec2_address
    command: shell.exec
    params:
      shell: bash
      working_dir: src
      script: |
        if [ -z "${instance_id}" ]; then
          exit 0
        fi
        # Ensure we use the latest private_ip_address, as it could change if the EC2 instance
        # has been stopped and started.
        ${activate_virtualenv}
        # Specify '--mode start' to ensure the remote instance is running.
        now=$(date +'%Y%m%d%H%M%S')
        aws_ec2_status_yml=aws_ec2_status.yml
        $python buildscripts/aws_ec2.py            \
          --imageId ${instance_id}                 \
          --mode start                             \
          --yamlFile $aws_ec2_status_yml           \
          --consoleOutputFile ec2_console_$now.log \
          --consoleScreenshotFile ec2_console_screen_shot_$now.jpg
        private_ip_address=$($python buildscripts/yaml_key_value.py --yamlFile $aws_ec2_status_yml --yamlKey private_ip_address)
        echo "private_ip_address: $private_ip_address" > private_ip_address.yml

  "update EC2 address": &update_ec2_address
    command: expansions.update
    params:
      file: src/private_ip_address.yml

  ### Process & archive remote EC2 artifacts ###
  "tar EC2 artifacts": &tar_ec2_artifacts
    command: shell.exec
    params:
      shell: bash
      working_dir: src
      script: |
        # Tar/zip artifacts on remote host.
        if [[ -z "${ec2_artifacts}" || -n "${ec2_ssh_failure}" ]]; then
          exit 0
        fi
        cmd="${tar|tar} czf ec2_artifacts.tgz ${ec2_artifacts}"
        ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
        ${activate_virtualenv}
        $python buildscripts/remote_operations.py          \
          --verbose                                        \
          --userHost $USER@${private_ip_address}           \
          --sshConnectionOptions "$ssh_connection_options" \
          --retries ${ssh_retries|0}                       \
          --commands "$cmd"

  "copy EC2 artifacts": &copy_ec2_artifacts
    command: shell.exec
    params:
      shell: bash
      working_dir: src
      script: |
        # Copy remote artifacts.
        if [[ -z "${ec2_artifacts}" || -n "${ec2_ssh_failure}" ]]; then
          exit 0
        fi
        ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
        ${activate_virtualenv}
        $python buildscripts/remote_operations.py          \
          --verbose                                        \
          --userHost $USER@${private_ip_address}           \
          --operation "copy_from"                          \
          --sshConnectionOptions "$ssh_connection_options" \
          --retries ${ssh_retries|0}                       \
          --file ec2_artifacts.tgz

  "cleanup EC2 instance": &cleanup_ec2_instance
    command: shell.exec
    params:
      shell: bash
      working_dir: src
      script: |
        # We do not terminate the EC2 instance if there was an ec2_ssh_failure.
        if [[ -z ${instance_id|""} || -n "${ec2_ssh_failure}" ]]; then
          exit 0
        fi
        ${activate_virtualenv}
        echo "Terminating $instance_id"
        aws_ec2=$($python buildscripts/aws_ec2.py --imageId ${instance_id} --mode terminate)
        echo "Terminated AMI EC2 instance: $aws_ec2"

  # The event logs on Windows are a useful diagnostic to have when determining if something bad
  # happened to the remote machine after it was repeatedly crashed during powercycle testing. For
  # example, the Application and System event logs have previously revealed that the mongod.exe
  # process abruptly exited due to not being able to open a file despite the process successfully
  # being restarted and responding to network requests.
  "gather remote event logs": &gather_remote_event_logs
    command: shell.exec
    params:
      shell: bash
      working_dir: src
      script: |
        if [[ "Windows_NT" != "$OS" || ! -f ${aws_ec2_yml|""} || -n "${ec2_ssh_failure}" ]]; then
          exit 0
        fi

        cmds="mkdir -p ${event_logpath}"
        cmds="$cmds; wevtutil qe Application /c:10000 /rd:true /f:Text > ${event_logpath}/application.log"
        cmds="$cmds; wevtutil qe Security    /c:10000 /rd:true /f:Text > ${event_logpath}/security.log"
        cmds="$cmds; wevtutil qe System      /c:10000 /rd:true /f:Text > ${event_logpath}/system.log"

        ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
        ${activate_virtualenv}
        $python buildscripts/remote_operations.py          \
          --verbose                                        \
          --userHost $USER@${private_ip_address}           \
          --sshConnectionOptions "$ssh_connection_options" \
          --retries ${ssh_retries|0}                       \
          --commands "$cmds"

  "gather remote mongo coredumps": &gather_remote_mongo_coredumps
    command: shell.exec
    params:
      shell: bash
      working_dir: "src"
      script: |
        if [[ ! -f ${aws_ec2_yml|""} || -n "${ec2_ssh_failure}" ]]; then
          exit 0
        fi
        ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
        remote_dir=${remote_dir|.}
        # Find all core files and move to $remote_dir
        cmds="core_files=\$(/usr/bin/find -H . \( -name '*.core' -o -name '*.mdmp' \) 2> /dev/null)"
        cmds="$cmds; if [ -z \"\$core_files\" ]; then exit 0; fi"
        cmds="$cmds; echo Found remote core files \$core_files, moving to \$(pwd)"
        cmds="$cmds; for core_file in \$core_files"
        cmds="$cmds; do base_name=\$(echo \$core_file | sed 's/.*\///')"
        cmds="$cmds;   if [ ! -f \$base_name ]; then mv \$core_file .; fi"
        cmds="$cmds; done"
        ${activate_virtualenv}
        $python buildscripts/remote_operations.py          \
          --verbose                                        \
          --userHost $USER@${private_ip_address}           \
          --sshConnectionOptions "$ssh_connection_options" \
          --retries ${ssh_retries}                         \
          --commands "$cmds"                               \
          --commandDir $remote_dir

  "copy remote mongo coredumps": &copy_remote_mongo_coredumps
    command: shell.exec
    params:
      shell: bash
      working_dir: "src"
      script: |
        if [[ ! -f ${aws_ec2_yml|""} || -n "${ec2_ssh_failure}" ]]; then
          exit 0
        fi
        ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
        remote_dir=${remote_dir|.}
        ${activate_virtualenv}
        $python buildscripts/remote_operations.py          \
          --verbose                                        \
          --userHost $USER@${private_ip_address}           \
          --operation "copy_from"                          \
          --sshConnectionOptions "$ssh_connection_options" \
          --retries ${ssh_retries}                         \
          --file "$remote_dir/*.core"                      \
          --file "$remote_dir/*.mdmp"
        # Since both type of core files do not exist on the same host, this command
        # will always return non-zero. As the core file retrieval is optional, we
        # always exit successfully.
        exit 0

  "archive remote EC2 artifacts": &archive_remote_ec2_artifacts
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/ec2_artifacts.tgz
      remote_file: ${project}/${build_variant}/${revision}/remote_ec2/remote_ec2_artifacts-${task_id}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Remote EC2 Artifacts - Execution ${execution}
      optional: true

  "archive remote EC2 monitor files": &archive_remote_ec2_monitor_files
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/ec2_monitor_files.tgz
      remote_file: ${project}/${build_variant}/${revision}/remote_ec2/remote_ec2_monitor-${task_id}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Remote EC2 Monitor - Execution ${execution}
      optional: true

  "gather EC2 console artifacts": &gather_ec2_console_artifacts
    command: shell.exec
    params:
      working_dir: src
      script: |
        ec2_console_files=$(ls ec2_console* 2> /dev/null)
        if [ -n "$ec2_console_files" ]; then
          ${tar|tar} czf ec2_console_files.tgz $ec2_console_files
        fi

  "archive EC2 console artifacts": &archive_ec2_console_artifacts
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/ec2_console_files.tgz
      remote_file: ${project}/${build_variant}/${revision}/ec2/ec2_console-${task_id}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/x-gzip}
      display_name: EC2 Console files - Execution ${execution}
      optional: true

  "save ec2 task artifacts":
    - *get_ec2_address
    - *update_ec2_address
    - *gather_remote_event_logs
    - *tar_ec2_artifacts
    - *copy_ec2_artifacts
    - *gather_remote_mongo_coredumps
    - *copy_remote_mongo_coredumps
    - *gather_ec2_console_artifacts
    - *archive_ec2_console_artifacts
    - *cleanup_ec2_instance
    - *archive_remote_ec2_artifacts
    - *archive_remote_ec2_monitor_files

  ### Process & archive local client logs ###
  "tar local client logs": &tar_local_client_logs
    command: shell.exec
    params:
      working_dir: src
      script: |
        client_logs=$(ls crud*.log fsm*.log 2> /dev/null)
        if [ ! -z "$client_logs" ]; then
          ${tar|tar} czf client-logs.tgz $client_logs
        fi

  "archive local client logs": &archive_local_client_logs
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/client-logs.tgz
      remote_file: ${project}/${build_variant}/${revision}/client_logs/mongo-client-logs-${task_id}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Client logs - Execution ${execution}
      optional: true

  "save local client logs":
    - *tar_local_client_logs
    - *archive_local_client_logs

  ### Clear and print OOM messages ###
  "clear OOM messages":
      command: shell.exec
      params:
        system_log: true
        script: |

          ulimit -a
          # Clear the dmesg ring buffer. The "post" phase will check dmesg for OOM messages.
          ${set_sudo}
          $sudo dmesg -c > /dev/null 2>&1
          if [ $? -eq 0 ]; then
            echo "Cleared the dmesg ring buffer"
          else
            echo "Could not clear the dmesg ring buffer"
          fi

  "print OOM messages":
    # Print out any Out of Memory killed process messages.
    command: shell.exec
    params:
      system_log: true
      working_dir: src # Temporary files created in src will be cleaned up in "pre".
      script: |
        ${set_sudo}
        # Use dmesg -T option, if supported, to display timestamps.
        dmesg=dmesg
        $sudo dmesg -T > /dev/null 2>&1
        if [ $? -eq 0 ]; then
          dmesg="dmesg -T"
        fi
        $sudo $dmesg 2> /dev/null > dmesg.txt
        if [ $? -ne 0 ]; then
          echo "Cannot check for OOM (Out of memory) killed processes on this platform"
          exit 0
        fi
        grep -iE '(Out of memory|OOM[- ]killer|Killed process)' dmesg.txt > oom.txt
        if [ -s oom.txt ]; then
          echo "OOM (Out of memory) killed processes detected"
          cat oom.txt
        else
          echo "No OOM (Out of memory) killed processes detected"
        fi

  ### Cleanup after the watchdog FUSE testing ###
  "cleanup FUSE watchdog":
    command: shell.exec
    params:
      working_dir: src
      script: |
        if [ -d /data/thrift ]; then
          rm -rf /data/thrift
        fi

        if [ -d /data/charybdefs ]; then
          rm -rf /data/charybdefs
        fi

  ### Process & archive mongo coredumps ###
  "gather mongo coredumps": &gather_mongo_coredumps
    command: shell.exec
    params:
      working_dir: "src"
      script: |
        # Find all core files and move to src
        core_files=$(/usr/bin/find -H .. \( -name "*.core" -o -name "*.mdmp" \) 2> /dev/null)
        for core_file in $core_files
        do
          base_name=$(echo $core_file | sed "s/.*\///")
          # Move file if it does not already exist
          if [ ! -f $base_name ]; then
            mv $core_file .
          fi
        done

  "tar mongo coredumps": &tar_mongo_coredumps
    command: archive.targz_pack
    params:
      target: "mongo-coredumps.tgz"
      source_dir: "src"
      include:
        - "./**.core"
        - "./**.mdmp" # Windows: minidumps

  "archive mongo coredumps": &archive_mongo_coredumps
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: mongo-coredumps.tgz
      remote_file: ${project}/${build_variant}/${revision}/coredumps/mongo-coredumps-${build_id}-${task_name}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Core Dumps - Execution ${execution}
      optional: true

  "save mongo coredumps":
  - *gather_mongo_coredumps
  - *tar_mongo_coredumps
  - *archive_mongo_coredumps

  ### Process & archive failed unittest artifacts ###
  "gather failed unittests": &gather_failed_unittests
    command: shell.exec
    params:
      working_dir: "src"
      script: |
        mkdir unittest_binaries || true
        # Find all core files
        core_files=$(/usr/bin/find -H . \( -name "dump_*.core" -o -name "*.mdmp" \) 2> /dev/null)
        for core_file in $core_files
        do
          # A core file name does not always have the executable name that generated it.
          # See http://stackoverflow.com/questions/34801353/core-dump-filename-gets-thread-name-instead-of-executable-name-with-core-pattern
          # On platforms with GDB, we get the binary name from core file
          gdb=/opt/mongodbtoolchain/gdb/bin/gdb
          if [ -f $gdb ]; then
            binary_file=$($gdb -batch --quiet -ex "core $core_file" 2> /dev/null | grep "Core was generated" | cut -f2 -d "\`" | cut -f1 -d "'" | cut -f1 -d " ")
            binary_file_locations=$binary_file
          else
            # Find the base file name from the core file name, note it may be truncated.
            # Remove leading 'dump_' and trailing '.<pid>.core' or '.<pid or time>.mdmp'
            binary_file=$(echo $core_file | sed "s/.*\///;s/dump_//;s/\..*\.core//;s/\..*\.mdmp//")
            # Locate the binary file. Since the base file name might be truncated, the find
            # may return more than 1 file.
            binary_file_locations=$(/usr/bin/find -H . -name "$binary_file*${exe}" 2> /dev/null)
          fi
          if [ -z "$binary_file_locations" ]; then
            echo "Cannot locate the unittest binary file ($binary_file) that generated the core file $core_file"
          fi
          for binary_file_location in $binary_file_locations
          do
            new_binary_file=unittest_binaries/$(echo $binary_file_location | sed "s/.*\///")
            if [ ! -f $new_binary_file ]; then
              mv $binary_file_location $new_binary_file
            fi
            # On Windows if a .pdb symbol file exists, include it in the archive.
            pdb_file=$(echo $binary_file_location | sed "s/\.exe/.pdb/")
            if [ -f $pdb_file ]; then
              new_pdb_file=unittest_binaries/$(echo $pdb_file | sed "s/.*\///")
              mv $pdb_file $new_pdb_file
            fi
          done
        done

  "tar failed unittests": &tar_failed_unittests
    command: archive.targz_pack
    params:
      target: "mongo-unittests.tgz"
      source_dir: "src/unittest_binaries"
      include:
        - "./*_test${exe}"

  "archive failed unittests": &archive_failed_unittests
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: mongo-unittests.tgz
      remote_file: ${project}/${build_variant}/${revision}/unittests/mongo-unittests-${build_id}-${task_name}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Unit tests - Execution ${execution}
      optional: true

  "save failed unittests":
  - *gather_failed_unittests
  - *tar_failed_unittests
  - *archive_failed_unittests

  "detect failed dbtest": &detect_failed_dbtest
    command: shell.exec
    params:
      working_dir: "src"
      script: |
        if [ -f resmoke_error_code ] && [ -f ../dbtest_unstripped.tgz ]; then
          echo "Task dbtest failed with error $(cat resmoke_error_code), archiving the unstripped binary"
          mv ../dbtest_unstripped.tgz ../dbtest.tgz
        fi

  "archive unstripped dbtest": &archive_unstripped_dbtest
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: dbtest.tgz
      remote_file: ${project}/${build_variant}/${revision}/dbtest/dbtest-${build_id}-${task_name}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: application/tar
      display_name: Unstripped dbtest binary - Execution ${execution}
      optional: true

  "save unstripped dbtest":
  - *detect_failed_dbtest
  - *archive_unstripped_dbtest

  ### Process & archive artifacts from hung processes ###
  "run hang analyzer":
    command: shell.exec
    params:
      working_dir: src
      script: |
        set -o verbose

        hang_analyzer_option="-o file -o stdout -p ${hang_analyzer_processes|dbtest,java,mongo,mongod,mongos,python,_test} -g bsondump,mongodump,mongoexport,mongofiles,mongoimport,mongoreplay,mongorestore,mongostat,mongotop"

        if [ ${hang_analyzer_dump_core|true} = true ]; then
          hang_analyzer_option="-c $hang_analyzer_option"
        fi

        ${activate_virtualenv}
        echo "Calling the hang analyzer: PATH=\"/opt/mongodbtoolchain/gdb/bin:$PATH\" $python buildscripts/hang_analyzer.py $hang_analyzer_option"
        PATH="/opt/mongodbtoolchain/gdb/bin:$PATH" $python buildscripts/hang_analyzer.py $hang_analyzer_option

        # Call hang_analyzer.py script for tasks that are running remote mongo processes
        if [ -n "${private_ip_address}" ]; then
          core_ext=core
          if [ "Windows_NT" = "$OS" ]; then
            core_ext=mdmp
          fi
          ssh_connection_options="${ssh_identity} ${ssh_connection_options}"
          # buildscripts must be installed in ${remote_dir} on the remote host.
          remote_dir=${remote_dir|.}

          # Copy mongoDB debug symbols to the remote host.
          debug_files=$(ls *.debug *.dSYM *.pdb 2> /dev/null)
          for debug_file in $debug_files
          do
            file_param="$file_param --file $debug_file"
          done
          if [ ! -z "$file_param" ]; then
            $python buildscripts/remote_operations.py          \
              --verbose                                        \
              --userHost $USER@${private_ip_address}           \
              --operation "copy_to"                            \
              --sshConnectionOptions "$ssh_connection_options" \
              --retries ${ssh_retries}                         \
              $file_param                                      \
              --remoteDir $remote_dir
          fi

          # Activate virtualenv on remote host. The virtualenv bin_dir is different for Linux and
          # Windows.
          bin_dir=$(find $VIRTUAL_ENV -name activate | sed -e "s,$VIRTUAL_ENV,,;s,activate,,;s,/,,g")
          cmds=". ${virtualenv_dir|venv}/$bin_dir/activate"
          # In the 'cmds' variable we pass to remote host, use 'python' instead of '$python' since
          # we don't want to evaluate the local python variable, but instead pass the python string
          # so the remote host will use the right python when the virtualenv is sourced.
          cmds="$cmds; cd ${remote_dir}"
          cmds="$cmds; PATH=\"/opt/mongodbtoolchain/gdb/bin:\$PATH\" python buildscripts/hang_analyzer.py $hang_analyzer_option"
          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries}                         \
            --commands "$cmds"

          $python buildscripts/remote_operations.py          \
            --verbose                                        \
            --userHost $USER@${private_ip_address}           \
            --operation "copy_from"                          \
            --sshConnectionOptions "$ssh_connection_options" \
            --retries ${ssh_retries}                         \
            --file "$remote_dir/debugger*.*"                 \
            --file "$remote_dir/*.$core_ext"
        fi

  "tar hang analyzer debugger files": &tar_hang_analyzer_debugger_files
    command: archive.targz_pack
    params:
      target: "src/mongo-hanganalyzer.tgz"
      source_dir: "src"
      include:
        - "./debugger*.*"

  "archive hang analyzer debugger files": &archive_hang_analyzer_debugger_files
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/mongo-hanganalyzer.tgz
      remote_file: ${project}/${build_variant}/${revision}/hanganalyzer/mongo-hanganalyzer-${build_id}-${task_name}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Hang Analyzer Output - Execution ${execution}
      optional: true

  "save hang analyzer debugger files":
  - *tar_hang_analyzer_debugger_files
  - *archive_hang_analyzer_debugger_files

  ### Process & archive disk statistic artifacts ###
  "tar disk statistics": &tar_disk_statistics
    command: archive.targz_pack
    params:
      target: "diskstats.tgz"
      source_dir: "./"
      include:
        - "./mongo-diskstats*"
        - "./mongo-diskstats*.csv"

  "archive disk statistics": &archive_disk_statistics
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: diskstats.tgz
      remote_file: ${project}/${build_variant}/${revision}/diskstats/mongo-diskstats-${task_id}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Disk Stats - Execution ${execution}
      optional: true

  "save disk statistics":
  - *tar_disk_statistics
  - *archive_disk_statistics

  ### Process & archive system resource artifacts ###
  "tar system resource information": &tar_system_resource_information
    command: archive.targz_pack
    params:
      target: "system-resource-info.tgz"
      source_dir: src
      include:
        - "./system_resource_info*"

  "archive system resource information": &archive_system_resource_information
    command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: system-resource-info.tgz
      remote_file: ${project}/${build_variant}/${revision}/systemresourceinfo/mongo-system-resource-info-${task_id}-${execution}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: System Resource Info - Execution ${execution}
      optional: true

  "save system resource information":
  - *tar_system_resource_information
  - *archive_system_resource_information

  ### Attach report & artifacts ###
  "attach scons config log":
    command: s3.put
    params:
      optional: true
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/build/scons/config.log
      remote_file: ${project}/${build_variant}/${revision}/artifacts/config-${build_id}.log
      bucket: mciuploads
      permissions: public-read
      content_type: text/plain
      display_name: config.log

  "attach report":
    command: attach.results
    params:
      file_location: ${report_file|src/report.json}

  "attach artifacts":
    command: attach.artifacts
    params:
      optional: true
      ignore_artifacts_for_spawn: false
      files:
        - ${archive_file|src/archive.json}

  "attach wiki page":
    - command: shell.exec
      params:
        script: |
          set -o errexit
          set -o verbose

          ${activate_virtualenv}
          $python -c 'import json; print(json.dumps([{
            "name": "Wiki: Running Tests from Evergreen Tasks Locally",
            "link": "https://github.com/mongodb/mongo/wiki/Running-Tests-from-Evergreen-Tasks-Locally",
            "visibility": "public",
            "ignore_for_fetch": True
          }]))' > wiki_page_location.json
    - command: attach.artifacts
      params:
        files:
          - wiki_page_location.json


# Pre task steps
pre:
  - func: "kill processes"
  - func: "cleanup environment"
  - func: "set task expansion macros"
  - func: "clear OOM messages"

# Post task steps
post:
  - func: "attach report"
  - func: "attach artifacts"
  - func: "save ec2 task artifacts"
  - func: "call BF Suggestion service"
  - func: "attach wiki page"
  - func: "kill processes"
  - func: "save local client logs"
  - func: "save mongo coredumps"
  - func: "save failed unittests"
  - func: "save hang analyzer debugger files"
  - func: "save disk statistics"
  - func: "save system resource information"
  - func: "print OOM messages"
  - func: "umount shared scons directory"
  - func: "cleanup FUSE watchdog"
  - func: "cleanup environment"

# Timeout steps
timeout:
  - func: "get EC2 address"
  - func: "update EC2 address"
  - func: "run hang analyzer"


#######################################
#               Tasks                 #
#######################################

tasks:

## compile - build all scons targets except unittests ##
- name: compile
  depends_on: []
  commands:
    - func: "build new tools"
    - func: "scons compile"
      vars:
        targets: >-
          archive-dist
          archive-dist-debug
          install-core
          install-tools
          distsrc-${ext|tgz}
          ${additional_targets|}
          ${msi_target|}
          ${mh_target|}
        task_compile_flags: >-
          --use-new-tools
          --build-mongoreplay="${build_mongoreplay}"
          --detect-odr-violations
          --install-mode=hygienic
          --separate-debug
          --legacy-tarball
    - command: shell.exec
      type: test
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          if [ "${is_patch}" = "true" ] && [ "${bypass_compile|false}" = "true" ]; then
            exit 0
          fi

          mv mongodb-src-*.${ext|tgz} distsrc.${ext|tgz}
          mv mongodb-*-debugsymbols.${ext|tgz} mongo-debugsymbols.tgz || true
          mv mongodb-*.${ext|tgz} mongodb-binaries.tgz

    # Tar unstripped dbtest, to be archived in case of failure
    - command: archive.targz_pack
      params:
        target: "dbtest_unstripped.tgz"
        source_dir: "src"
        include:
          - "./dbtest*"

    - command: shell.exec
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          if [ "${is_patch}" = "true" ] && [ "${bypass_compile|false}" = "true" ]; then
            exit 0
          fi
          ${activate_virtualenv}
          if [ "${has_packages|}" = "true" ] ; then
            cd buildscripts
            $python ${packager_script} --prefix `pwd`/.. --distros ${packager_distro} --tarball `pwd`/../mongodb-binaries.tgz -s ${version} -m HEAD -a ${packager_arch}
            cd ..
          fi

          # Create separate shell archive
          mkdir -p shell-archive/build
          cd shell-archive
          ${platform_decompress|tar xzvf} ../mongodb-binaries.tgz
          find . -mindepth 3 ! -name "mongo${exe}" -type f -exec rm {} \; # delete bin/* except bin/mongo
          $python ../buildscripts/make_archive.py -o mongodb-shell.${ext|tgz} $(find mongodb-* -type f)
          cd ..

          # Create separate cryptd archive if mongocryptd is in the tarball
          mkdir -p cryptd-archive/build
          cd cryptd-archive
          ${platform_decompress|tar xzvf} ../mongodb-binaries.tgz
          find . -mindepth 3 ! -name "mongocryptd${exe}" -type f -exec rm {} \; # delete bin/* except bin/mongocryptd
          if [ $(find . -name mongocryptd${exe} | wc -l) -eq 1 ] ; then
            $python ../buildscripts/make_archive.py -o mongodb-cryptd.${ext|tgz} $(find mongodb-* -type f)

            # Validate that this build_variant is listed as a known enterprise task for mongocryptd
            PATH=$PATH:$HOME $python ../buildscripts/validate_mongocryptd.py --variant "${build_variant}" ../etc/evergreen.yml
          fi
          cd ..

    - command: archive.targz_pack
      params:
        target: "packages.tgz"
        source_dir: "src"
        include:
          - "repo/**"

    - command: archive.targz_pack
      params:
        target: "artifacts.tgz"
        source_dir: "src"
        include:
          - "src/mongo/db/modules/enterprise/jstests/**"
          - "compile_expansions.yml"
          - "src/mongo/db/modules/subscription/jstests/**"
          - "src/mongo/db/modules/enterprise/docs/**"
          - "*.exe"
          - "jstests/**"
          - "pytests/**"
          - "./test*"
          - "./mongobridge*"
          - "./mongotmock*"
          - "./wt*"
          - "buildscripts/**"
          - "*Example"
          - "*Test"
          - "./**.pdb"
          - "./**.msi"
          - "./etc/pip/**"
          - "./etc/scons/**"
          - "./etc/*san.suppressions"
          - "./etc/repo_config.yaml"
          - "./build/**.gcno"
          - "src/mongo/util/options_parser/test_config_files/**"
          - "src/mongo/client/sdam/json_tests/**"
          - "library_dependency_graph.json"
          - "src/third_party/JSON-Schema-Test-Suite/tests/draft4/**"
          - "src/third_party/mock_ocsp_responder/**"
          - "bypass_compile_expansions.yml"
          - "patch_files.txt"
          - "artifacts.json"
        exclude_files:
          - "*_test.pdb"

    - func: "upload debugsymbols"
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/mongodb-binaries.tgz
        remote_file: ${mongo_binaries}
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Binaries
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/cryptd-archive/mongodb-cryptd.${ext|tgz}
        remote_file: ${mongo_cryptd}
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: CryptD Binaries
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/mh-binaries.${ext|tgz}
        remote_file: ${mh_archive}
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: MH Binaries
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/mh-debugsymbols.${ext|tgz}
        remote_file: ${mh_debugsymbols}
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: MH Debuginfo
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/shell-archive/mongodb-shell.${ext|tgz}
        remote_file: ${mongo_shell}
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Shell
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: artifacts.tgz
        remote_file: ${project}/${build_variant}/${revision}/artifacts/${build_id}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: application/tar
        display_name: Artifacts
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: packages.tgz
        remote_file: ${project}/${build_variant}/${revision}/artifacts/${build_id}-packages.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: application/tar
        display_name: Packages
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/distsrc.${ext|tgz}
        remote_file: ${project}/${build_variant}/${revision}/sources/mongo-src-${build_id}.${ext|tgz}
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/gzip}
        display_name: Source tarball
        # We only need to upload the source tarball from one of the build variants
        # because it should be the same everywhere, so just use rhel70/windows.
        build_variants: [rhel70]
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/scons_cache.log
        content_type: text/plain
        remote_file: ${project}/${build_variant}/${revision}/scons-cache-${build_id}-${execution}.log
        bucket: mciuploads
        permissions: public-read
        display_name: SCons cache debug log
    # For patch builds that bypass compile, we upload links to pre-existing tarballs, except for the
    # artifacts.tgz.
    - command: attach.artifacts
      params:
        optional: true
        ignore_artifacts_for_spawn: false
        files:
          - src/artifacts.json

## compile_core_tools - minimal version of compile used for commit queue ##
- name: compile_core_tools
  depends_on: []
  commands:
    - func: "build new tools"
    - func: "scons compile"
      vars:
        targets: install-core install-tools archive-dist ${mh_target|}
        compiling_for_test: true
        additional_targets: ""
        task_compile_flags: >-
          --use-new-tools
          --build-mongoreplay="${build_mongoreplay}"
          --detect-odr-violations
          --install-mode=hygienic
          --separate-debug

- name: compile_ninja
  commands:
    - func: "scons compile"
      vars:
        task_compile_flags: >-
          --ninja
        targets:
          new.build.ninja
    - command: shell.exec
      params:
        working_dir: src
        shell: bash
        script: ninja -f new.build.ninja

## compile_all - build all scons targets ##
- name: compile_all
  commands:
    - func: "build new tools"
    - func: "scons compile"
      vars:
        targets: all
        compiling_for_test: true
        task_compile_flags: >-
          --use-new-tools
          --build-mongoreplay="${build_mongoreplay}"
          --detect-odr-violations
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/library_dependency_graph.json
        remote_file: ${project}/${build_variant}/${revision}/library_dependency_graph.${build_id}.json
        bucket: mciuploads
        permissions: public-read
        content_type: application/json
        display_name: Library Dependency Graph (library_dependency_graph.json)
        build_variants: [enterprise-rhel-70-64-bit-kitchen-sink] # This must be the Dagger variant

## unittests - run unittests ##
- name: unittests
  commands:
    - func: "scons compile"
      vars:
        targets: unittests
        task_compile_flags: >-
          --detect-odr-violations
        compiling_for_test: true
    - func: "run diskstats"
    - func: "monitor process threads"
    - func: "collect system resource info"
    - func: "run tests"
      vars:
        resmoke_args: --suites=unittests

##compile_libfuzzertests - build libfuzzertests ##
- name: compile_libfuzzertests
  commands:
    - func: "scons compile"
      vars:
        targets: libfuzzer_tests
        task_compile_flags: >-
          --detect-odr-violations
        compiling_for_test: true
    - command: shell.exec
      params:
        working_dir: "src"
        script: |
          set -o errexit
          set -o verbose

          tar cfvz libfuzzer-tests.tgz --files-from=build/libfuzzer_tests.txt
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: "src/libfuzzer-tests.tgz"
        remote_file: "${project}/libfuzzer-tests/${build_variant}/${revision}/libfuzzer-tests.tgz"
        bucket: mciuploads
        permissions: public-read
        content_type: application/tar
        display_name: "LibFuzzer Tests"

## libfuzzertests - run libfuzzertests ##
- name: libfuzzertests
  commands:
    - func: "fetch corpus"
    - func: "extract corpus"
    - func: "run tests"
      vars:
        resmoke_args: --suites=libfuzzer

- name: server_discovery_and_monitoring_json_test
  commands:
    - func: "scons compile"
      vars:
        targets: "sdam_json_test_runner"
        task_compile_flags: >-
          --detect-odr-violations
        compiling_for_test: true
    - func: "run tests"
      vars:
        resmoke_args: --suites=sdam_json_test

## dbtest ##
- name: dbtest
  commands:
    - func: "scons compile"
      vars:
        targets: dbtest
        task_compile_flags: >-
          --detect-odr-violations
        compiling_for_test: true
    # Tar unstripped dbtest, to be archived in case of failure
    - command: archive.targz_pack
      params:
        target: "dbtest_unstripped.tgz"
        source_dir: "src"
        include:
          - "./dbtest*"
    - func: "run diskstats"
    - func: "monitor process threads"
    - func: "collect system resource info"
    - func: "run tests"
      vars:
        resmoke_args: --suites=dbtest --storageEngine=wiredTiger

## embedded_sdk_build_and_test_* - build the embedded-dev and embedded-test targets only ##

- name: embedded_sdk_build_cdriver
  commands:
    - command: shell.exec
      params:
        script: |
          set -o errexit
          set -o verbose

          VERSION=${version}
          WORKDIR=${workdir}

          # build in a different directory then we run tests so that we can verify that the linking
          # of tests are not relying any built in absolute paths
          FINAL_PREFIX=$WORKDIR/src/build/mongo-embedded-sdk-$VERSION
          BUILD_PREFIX=$FINAL_PREFIX-tmp

          rm -rf mongo-c-driver

          # NOTE: If you change the C Driver version here, also change the substitution in the CocoaPod podspec below in the apple builder.
          git clone --branch r1.13 --depth 1 https://github.com/mongodb/mongo-c-driver.git
          cd mongo-c-driver

          # Fixup VERSION so we don't end up with -dev on it. Remove this once we are building a stable version and CDRIVER-2861 is resolved.
          cp -f VERSION_RELEASED VERSION_CURRENT

          trap "cat CMakeFiles/CMakeOutput.log" EXIT
          export ${compile_env|}
          ${cmake_path|/opt/cmake/bin/cmake} -DCMAKE_INSTALL_PREFIX=$BUILD_PREFIX -DENABLE_SHM_COUNTERS=OFF -DENABLE_SNAPPY=OFF -DENABLE_AUTOMATIC_INIT_AND_CLEANUP=OFF -DENABLE_TESTS=OFF -DENABLE_EXAMPLES=OFF -DENABLE_STATIC=OFF -DCMAKE_OSX_DEPLOYMENT_TARGET=${cdriver_cmake_osx_deployment_target|} ${cdriver_cmake_flags}
          trap - EXIT # cancel the previous trap '...' EXIT
          make install VERBOSE=1

          # TODO: Remove this when we upgrade to a version of the C driver that has CDRIVER-2854 fixed.
          mkdir -p $BUILD_PREFIX/share/doc/mongo-c-driver
          cp COPYING $BUILD_PREFIX/share/doc/mongo-c-driver
          cp THIRD_PARTY_NOTICES $BUILD_PREFIX/share/doc/mongo-c-driver
          if [ -d $BUILD_PREFIX/Frameworks ]; then

              # We need to account for the fact that on the Darwin
              # mobile platforms, things shouldn't go to the Resources
              # directory but on macOS they should. If the Resources
              # directory is there (it should be for the plist file),
              # then use it, otherwise just use the framework root.
              if [ -e $BUILD_PREFIX/Frameworks/mongoc.framework/Resources ]; then
                  cp COPYING $BUILD_PREFIX/Frameworks/mongoc.framework/Resources
                  cp THIRD_PARTY_NOTICES $BUILD_PREFIX/Frameworks/mongoc.framework/Resources
                  plutil -insert MinimumOSVersion -string ${cdriver_cmake_osx_deployment_target} $BUILD_PREFIX/Frameworks/mongoc.framework/Resources/Info.plist
              else
                  cp COPYING $BUILD_PREFIX/Frameworks/mongoc.framework
                  cp THIRD_PARTY_NOTICES $BUILD_PREFIX/Frameworks/mongoc.framework
                  plutil -insert MinimumOSVersion -string ${cdriver_cmake_osx_deployment_target} $BUILD_PREFIX/Frameworks/mongoc.framework/Info.plist
              fi

              if [ -e $BUILD_PREFIX/Frameworks/bson.framework/Resources ]; then
                  cp COPYING $BUILD_PREFIX/Frameworks/bson.framework/Resources
                  cp THIRD_PARTY_NOTICES $BUILD_PREFIX/Frameworks/bson.framework/Resources
                  plutil -insert MinimumOSVersion -string ${cdriver_cmake_osx_deployment_target} $BUILD_PREFIX/Frameworks/bson.framework/Resources/Info.plist
              else
                  cp COPYING $BUILD_PREFIX/Frameworks/bson.framework
                  cp THIRD_PARTY_NOTICES $BUILD_PREFIX/Frameworks/bson.framework
                  plutil -insert MinimumOSVersion -string ${cdriver_cmake_osx_deployment_target} $BUILD_PREFIX/Frameworks/bson.framework/Info.plist
              fi

          fi

          # CMake doesn't seem to do the dSYM for us for the framework
          if [ -e $BUILD_PREFIX/Frameworks ]; then
              pushd $BUILD_PREFIX/Frameworks
              xcrun dsymutil -num-threads=1 -o bson.framework.dSYM bson.framework/bson
              xcrun strip -Sx bson.framework/bson
              xcrun dsymutil -num-threads=1 -o mongoc.framework.dSYM mongoc.framework/mongoc
              xcrun strip -Sx mongoc.framework/mongoc
              popd
          fi

          mv $BUILD_PREFIX $FINAL_PREFIX

- name: embedded_sdk_install_dev
  commands:
    - func: "scons compile"
      vars:
        targets: install-embedded-dev
        task_compile_flags: &embedded_sdk_compile_flags >-
          --allocator=system
          --dbg=off
          --enable-free-mon=off
          --enable-http-client=off
          --install-mode=hygienic
          --js-engine=none
          --opt=size
          --separate-debug
          --ssl=off
          --use-system-mongo-c=on
          --wiredtiger=off
          DESTDIR='$BUILD_ROOT/mongo-embedded-sdk-$MONGO_VERSION'
          CPPPATH='$BUILD_ROOT/mongo-embedded-sdk-$MONGO_VERSION/include/libbson-1.0 $BUILD_ROOT/mongo-embedded-sdk-$MONGO_VERSION/include/libmongoc-1.0'
        task_compile_flags_extra: >-
          --link-model=dynamic-sdk

- name: embedded_sdk_s3_put
  commands:
    # Not using archive.targz_pack here because I can't get it to work.
    - command: shell.exec
      params:
        working_dir: "src/build"
        script: |
          set -o errexit
          set -o verbose

          cat <<EOF > mongo-embedded-sdk-${version}/README-Licenses.txt
          The software accompanying this file is Copyright (C) 2018 MongoDB, Inc. and
          is licensed to you on the terms set forth in the following files:
            - mongo-c-driver: share/doc/mongo-c-driver/COPYING
            - mongo_embedded: share/doc/mongo_embedded/LICENSE-Embedded.txt
            - mongoc_embedded: share/doc/mongo_embedded/LICENSE-Embedded.txt
          EOF

          tar cfvz embedded-sdk.tgz mongo-embedded-sdk-${version}

    # Upload it so we can download from EVG.
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: "src/build/embedded-sdk.tgz"
        remote_file: ${project}/embedded-sdk/${build_variant}/${revision}/mongo-embedded-sdk-${version}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: application/tar
        display_name: "Embedded SDK Tar Archive"

- name: embedded_sdk_install_tests
  commands:
    - func: "scons compile"
      vars:
        targets: install-embedded-test
        compiling_for_test: true
        task_compile_flags: *embedded_sdk_compile_flags
        task_compile_flags_extra: >-
          --link-model=dynamic

- name: embedded_sdk_tests_s3_put
  commands:
    # Not using archive.targz_pack here because I can't get it to work.
    - command: shell.exec
      params:
        working_dir: "src/build"
        script: |
          set -o errexit
          set -o verbose

          tar cfvz embedded-sdk-tests.tgz mongo-embedded-sdk-${version}

    # Upload it so we can download from EVG.
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: "src/build/embedded-sdk-tests.tgz"
        remote_file: ${project}/embedded-sdk-test/${build_variant}/${revision}/mongo-embedded-sdk-test-${version}.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: application/tar
        display_name: "Embedded SDK Tests Tar Archive"

- name: embedded_sdk_run_tests
  commands:
    - command: shell.exec
      type: test
      params:
        working_dir: src
        script: |
          set -o verbose
          set -o errexit

          ${activate_virtualenv}
          "build/mongo-embedded-sdk-${version}/bin/mongo_embedded_test"
          "build/mongo-embedded-sdk-${version}/bin/mongoc_embedded_test"

    # If this is a patch build, blow away the file so our subsequent and optional s3.put
    # doesn't run. That way, we won't overwrite the latest part in our patches.
    - command: shell.exec
      params:
        working_dir: "src/build"
        script: |
          set -o errexit
          set -o verbose

          if [ "${is_patch}" = "true" ]; then
            rm -f src/build/embedded-sdk.tgz
          fi

- name: embedded_sdk_s3_put_latest
  commands:
    # A second put, this time to -latest, to give devs a reasonable
    # way to get the most recent build.
    - command: s3.put
      params:
        visibility: none
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: "src/build/embedded-sdk.tgz"
        remote_file: ${project}/embedded-sdk/mongo-${build_variant}-latest.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/x-gzip}

- name: embedded_sdk_tests_s3_put_latest
  commands:
    # A second put, this time to -latest, to give devs a reasonable
    # way to get the most recent build.
    - command: s3.put
      params:
        visibility: none
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: "src/build/embedded-sdk-tests.tgz"
        remote_file: ${project}/embedded-sdk-test/mongo-${build_variant}-latest.tgz
        bucket: mciuploads
        permissions: public-read
        content_type: ${content_type|application/x-gzip}

- name: stitch_support_create_lib
  commands:
    - func: "scons compile"
      vars:
        targets: install-stitch-support install-stitch-support-debug install-stitch-support-dev
        task_compile_flags: >-
          --install-mode=hygienic
          --dbg=off
          --link-model=dynamic-sdk
          --enable-free-mon=off
          --ssl=off
          --enable-http-client=off
          --modules=
          --separate-debug
          DESTDIR='$BUILD_ROOT/stitch-support-lib-$MONGO_VERSION'
    - command: shell.exec
      params:
        working_dir: "src/build"
        script: |
          set -o errexit
          set -o verbose

          tar cfvz stitch-support.tgz stitch-support-lib-${version}
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: "src/build/stitch-support.tgz"
        remote_file: "${project}/stitch-support/${build_variant}/${revision}/stitch-support-${version}.tgz"
        bucket: mciuploads
        permissions: public-read
        content_type: application/tar
        display_name: "Stitch Support Library"

- name: stitch_support_install_tests
  commands:
    - func: "scons compile"
      vars:
        targets: install-stitch-support-test
        compiling_for_test: true
        task_compile_flags: >-
          --install-mode=hygienic
          --dbg=off
          --enable-free-mon=off
          --ssl=off
          --enable-http-client=off
          --modules=
          --separate-debug
          DESTDIR='$BUILD_ROOT/stitch-support-lib-$MONGO_VERSION'

- name: stitch_support_run_tests
  commands:
    - command: shell.exec
      type: test
      params:
        working_dir: src
        script: |
          set -o errexit
          set -o verbose

          "build/stitch-support-lib-${version}/bin/stitch_support_test"

## lint ##
- name: lint_pylinters
  commands:
    - command: timeout.update
      params:
        # 40 minutes
        exec_timeout_secs: 2400
    - command: manifest.load
    - func: "git get project"
    - func: "set task expansion macros"
    - func: "set up virtualenv"
    - func: "upload pip requirements"
    - func: "scons lint"
      vars:
        targets: lint-pylinters

- name: lint_clang_format
  commands:
    - command: timeout.update
      params:
        # 40 minutes
        exec_timeout_secs: 2400
    - command: manifest.load
    - func: "git get project"
    - func: "set task expansion macros"
    - func: "set up virtualenv"
    - func: "upload pip requirements"
    - func: "scons lint"
      vars:
        targets: lint-clang-format

- name: lint_eslint
  commands:
    - command: timeout.update
      params:
        # 40 minutes
        exec_timeout_secs: 2400
    - command: manifest.load
    - func: "git get project"
    - func: "set task expansion macros"
    - func: "set up virtualenv"
    - func: "upload pip requirements"
    - func: "scons lint"
      vars:
        targets: lint-eslint

- name: lint_cpplint
  commands:
    - command: timeout.update
      params:
        # 40 minutes
        exec_timeout_secs: 2400
    - command: manifest.load
    - func: "git get project"
    - func: "set task expansion macros"
    - func: "set up virtualenv"
    - func: "upload pip requirements"
    - func: "scons lint"
      vars:
        targets: lint-lint.py

- name: lint_yaml
  depends_on: []
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "set task expansion macros"
  - func: "set up virtualenv"
  - func: "upload pip requirements"
  - command: shell.exec
    type: test
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose

        ${activate_virtualenv}
        find buildscripts etc jstests -name '*.y*ml' -exec yamllint -c etc/yamllint_config.yml {} +

- name: burn_in_tests_gen
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "set task expansion macros"
  - func: "set up virtualenv"
  - func: "upload pip requirements"
  - func: "configure evergreen api credentials"
  - command: shell.exec
    type: test
    params:
      working_dir: src
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${activate_virtualenv}
        # Capture a list of new and modified tests. The expansion macro burn_in_tests_build_variant
        # is used to for finding the associated tasks from a different build varaint than the
        # burn_in_tests_gen task executes on.
        build_variant_opts="--build-variant=${build_variant}"
        if [ -n "${burn_in_tests_build_variant|}" ]; then
          build_variant_opts="--build-variant=${burn_in_tests_build_variant} --run-build-variant=${build_variant}"
        fi
        burn_in_args="$burn_in_args --repeat-tests-min=2 --repeat-tests-max=1000 --repeat-tests-secs=600"
        # Evergreen executable is in $HOME.
        PATH=$PATH:$HOME $python buildscripts/burn_in_tests.py --project=${project} $build_variant_opts --distro=${distro_id} --generate-tasks-file=burn_in_tests_gen.json $burn_in_args --verbose
  - command: archive.targz_pack
    params:
      target: src/burn_in_tests_gen.tgz
      source_dir: src
      include:
        - burn_in_tests_gen.json

  - command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/burn_in_tests_gen.tgz
      remote_file: ${project}/${build_variant}/${revision}/burn_in_tests_gen/burn_in_tests_gen-${build_id}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Burn_in_tests Task Config - Execution ${execution}
  - command: generate.tasks
    params:
      files:
        - src/burn_in_tests_gen.json

- name: burn_in_tests_multiversion_gen
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "set task expansion macros"
  - func: "set up virtualenv"
  - func: "upload pip requirements"
  - func: "configure evergreen api credentials"
  - command: shell.exec
    type: test
    params:
      working_dir: src
      shell: bash
      script: |
        set -o errexit
        set -o verbose

        ${activate_virtualenv}
        # Capture a list of new and modified tests. The expansion macro burn_in_tests_build_variant
        # is used for finding the associated tasks from a different build variant than the
        # burn_in_tests_multiversion_gen task executes on.
        build_variant_opts="--build-variant=${build_variant}"
        if [ -n "${burn_in_tests_build_variant|}" ]; then
          build_variant_opts="--build-variant=${burn_in_tests_build_variant} --run-build-variant=${build_variant}"
        fi

        burn_in_args="$burn_in_args"
        # Evergreen executable is in $HOME.
        PATH=$PATH:$HOME $python buildscripts/burn_in_tests.py --task_id=${task_id} --project=${project} $build_variant_opts --distro=${distro_id} --generate-tasks-file=burn_in_tests_multiversion_gen.json $burn_in_args --use-multiversion --verbose

  - command: archive.targz_pack
    params:
      target: src/burn_in_tests_multiversion_gen.tgz
      source_dir: src
      include:
        - burn_in_tests_multiversion_gen.json

  - command: archive.targz_pack
    params:
      target: generate_tasks_config.tgz
      source_dir: src/generated_resmoke_config
      include:
        - "*"

  - command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: generate_tasks_config.tgz
      remote_file: ${project}/${build_variant}/${revision}/generate_tasks/burn_in_tests_multiversion_gen-${build_id}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      optional: true
      display_name: Generated Multiversion Resmoke.py Suite Config - Execution ${execution}

  - command: s3.put
    params:
      aws_key: ${aws_key}
      aws_secret: ${aws_secret}
      local_file: src/burn_in_tests_multiversion_gen.tgz
      remote_file: ${project}/${build_variant}/${revision}/generate_tasks/burn_in_tests_multiversion_gen_config-${build_id}.tgz
      bucket: mciuploads
      permissions: public-read
      content_type: ${content_type|application/gzip}
      display_name: Burn_in_tests Task Config - Execution ${execution}

  - command: generate.tasks
    params:
      files:
        - src/burn_in_tests_multiversion_gen.json

## initial sync multiversion fuzzer ##
- <<: *jstestfuzz_template
  name: initial_sync_multiversion_fuzzer_gen
  tags: ["multiversion_fuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 10
      num_tasks: 15
      npm_command: initsync-fuzzer
      suite: initial_sync_multiversion_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: initial_sync_multiversion_fuzzer
      task_path_suffix: "/data/multiversion"

## initial sync generational fuzzer ##
- <<: *jstestfuzz_template
  name: initial_sync_fuzzer_gen
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 10
      num_tasks: 5
      npm_command: initsync-fuzzer
      suite: initial_sync_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: initial_sync_fuzzer

## Standalone generational fuzzer for multiversion aggregation pipelines ##
- <<: *jstestfuzz_template
  name: aggregation_multiversion_fuzzer_gen
  tags: ["aggfuzzer", "common", "multiversion"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      npm_command: agg-fuzzer
      name: aggregation_multiversion_fuzzer
      task_path_suffix: "/data/multiversion"

## Standalone generational fuzzer for multiversion aggregation expressions ##
- <<: *jstestfuzz_template
  name: aggregation_expression_multiversion_fuzzer_gen
  tags: ["aggfuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      npm_command: agg-expr-fuzzer
      name: aggregation_expression_multiversion_fuzzer
      task_path_suffix: "/data/multiversion"

## Standalone generational fuzzer for checking optimized and unoptimized expression equivalence
- <<: *jstestfuzz_template
  name: aggregation_expression_optimization_fuzzer_gen
  tags: ["aggfuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      jstestfuzz_vars: --diffTestingMode optimization
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      npm_command: agg-expr-fuzzer
      name: aggregation_expression_optimization_fuzzer

## Standalone generational fuzzer for checking optimized and unoptimized aggregation pipelines
- <<: *jstestfuzz_template
  name: aggregation_optimization_fuzzer_gen
  tags: ["aggfuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      jstestfuzz_vars: --diffTestingMode optimization
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      npm_command: agg-fuzzer
      name: aggregation_optimization_fuzzer

## Standalone fuzzer for checking wildcard index correctness ##
- <<: *jstestfuzz_template
  name: aggregation_wildcard_fuzzer_gen
  tags: ["aggfuzzer", "common", "wildcard"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      jstestfuzz_vars: --diffTestingMode wildcard
      npm_command: agg-fuzzer
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: aggregation_wildcard_fuzzer

## jstestfuzz standalone fuzzer for checking find and aggregate equivalence ##
- <<: *jstestfuzz_template
  name: query_fuzzer_standalone_gen
  tags: ["query_fuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      jstestfuzz_vars: --diffTestingMode standalone
      npm_command: query-fuzzer
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: query_fuzzer_standalone

## jstestfuzz sharded fuzzer for checking find and aggregate equivalence ##
- <<: *jstestfuzz_template
  name: query_fuzzer_sharded_gen
  tags: ["query_fuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      jstestfuzz_vars: --diffTestingMode sharded
      npm_command: query-fuzzer
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: query_fuzzer_sharded

## jstestfuzz standalone update generational fuzzer ##
- <<: *jstestfuzz_template
  name: update_fuzzer_gen
  tags: ["updatefuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      npm_command: update-fuzzer
      suite: generational_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: update_fuzzer
      task_path_suffix: "/data/multiversion"

## jstestfuzz replication update generational fuzzer ##
- <<: *jstestfuzz_template
  name: update_fuzzer_replication_gen
  tags: ["updatefuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      npm_command: update-fuzzer
      suite: generational_fuzzer_replication
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: update_fuzzer_replication
      task_path_suffix: "/data/multiversion"

## rollback multiversion fuzzer ##
- <<: *jstestfuzz_template
  name: rollback_multiversion_fuzzer_gen
  tags: ["multiversion_fuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 3
      num_tasks: 5
      npm_command: rollback-fuzzer
      suite: rollback_multiversion_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: rollback_multiversion_fuzzer
      task_path_suffix: "/data/multiversion"

## rollback generational fuzzer ##
- <<: *jstestfuzz_template
  name: rollback_fuzzer_gen
  tags: ["rollbackfuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 3
      num_tasks: 5
      npm_command: rollback-fuzzer
      suite: rollback_fuzzer
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: rollback_fuzzer

## rollback generational fuzzer with clean shutdowns ##
- <<: *jstestfuzz_template
  name: rollback_fuzzer_clean_shutdowns_gen
  tags: ["rollbackfuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 1
      num_tasks: 4
      jstestfuzz_vars: --numLinesPerFile 300 --maxLinesBetweenEvents 50
      npm_command: rollback-fuzzer
      suite: rollback_fuzzer_clean_shutdowns
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: rollback_fuzzer_clean_shutdowns

## rollback generational fuzzer with unclean shutdowns ##
- <<: *jstestfuzz_template
  name: rollback_fuzzer_unclean_shutdowns_gen
  tags: ["rollbackfuzzer"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 1
      num_tasks: 4
      jstestfuzz_vars: --numLinesPerFile 300 --maxLinesBetweenEvents 50
      npm_command: rollback-fuzzer
      suite: rollback_fuzzer_unclean_shutdowns
      resmoke_args: "--mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: rollback_fuzzer_unclean_shutdowns

## jstestfuzz ##
- <<: *jstestfuzz_template
  name: jstestfuzz_gen
  tags: ["jstestfuzz", "common"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      npm_command: jstestfuzz
      name: jstestfuzz

## jstestfuzz concurrent ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_gen
  tags: ["jstestfuzz", "common"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 15
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz
      resmoke_args: --storageEngine=wiredTiger --numClientsPerFixture=10
      name: jstestfuzz_concurrent

## jstestfuzz concurrent replica set ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_replication_gen
  tags: ["jstestfuzz", "common", "repl"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 15
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_replication
      resmoke_args: --storageEngine=wiredTiger --numClientsPerFixture=10
      name: jstestfuzz_concurrent_replication

## jstestfuzz concurrent replica set with logical session ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_replication_session_gen
  tags: ["jstestfuzz", "session"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 15
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_replication_session
      resmoke_args: --storageEngine=wiredTiger --numClientsPerFixture=10
      name: jstestfuzz_concurrent_replication_session

## jstestfuzz concurrent sharded cluster ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_sharded_gen
  tags: ["jstestfuzz", "common"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 15
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded
      resmoke_args: --storageEngine=wiredTiger --numClientsPerFixture=10
      name: jstestfuzz_concurrent_sharded

## jstestfuzz concurrent sharded cluster causal consistency ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_sharded_causal_consistency_gen
  tags: ["jstestfuzz", "causal"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 15
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_causal_consistency
      resmoke_args: --storageEngine=wiredTiger --numClientsPerFixture=10
      name: jstestfuzz_concurrent_sharded_causal_consistency

## jstestfuzz concurrent sharded cluster continuous stepdown ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_sharded_continuous_stepdown_gen
  tags: ["jstestfuzz"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 2
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_continuous_stepdown
      resmoke_args: --storageEngine=wiredTiger --numClientsPerFixture=10
      name: jstestfuzz_concurrent_sharded_continuous_stepdown

## jstestfuzz concurrent sharded cluster with logical session ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_sharded_session_gen
  tags: ["jstestfuzz", "session"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 15
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_session
      resmoke_args: --storageEngine=wiredTiger --numClientsPerFixture=10
      name: jstestfuzz_concurrent_sharded_session

# jstestfuzz interrupt #
- <<: *jstestfuzz_template
  name: jstestfuzz_interrupt_gen
  tags: ["jstestfuzz", "interrupt"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_interrupt
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_interrupt

# jstestfuzz interrupt #
- <<: *jstestfuzz_template
  name: jstestfuzz_interrupt_replication_gen
  tags: ["jstestfuzz", "interrupt"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_interrupt_replication
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_interrupt_replication

# jstestfuzz interrupt with flow control engaged #
- <<: *jstestfuzz_template
  name: jstestfuzz_interrupt_replication_flow_control_gen
  tags: ["jstestfuzz", "interrupt", "flow_control"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 30
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_interrupt_replication
      resmoke_args: "--flowControlTicketOverride=1 --storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_interrupt_replication_flow_control

## jstestfuzz sharded cluster continuous stepdown with flow control engaged ##
- <<: *jstestfuzz_template
  name: jstestfuzz_sharded_continuous_stepdown_flow_control_gen
  tags: ["jstestfuzz", "flow_control"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_continuous_stepdown
      # TODO(SERVER-44896): Remove enableTwoPhaseIndexBuild from server parameters when two phase
      # index builds work with this flow control setting.
      resmoke_args: >-
          --flowControlTicketOverride=3
          --storageEngine=wiredTiger
          --mongodSetParameters="{logComponentVerbosity: {command: 2}, enableTwoPhaseIndexBuild: false}"
      name: jstestfuzz_sharded_continuous_stepdown_flow_control

## jstestfuzz concurrent sharded cluster continuous stepdown with flow control engaged ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_sharded_continuous_stepdown_flow_control_gen
  tags: ["jstestfuzz", "flow_control"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 2
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_continuous_stepdown
      # TODO(SERVER-44896): Remove enableTwoPhaseIndexBuild from server parameters when two phase
      # index builds work with this flow control setting.
      resmoke_args: >-
          --flowControlTicketOverride=30
          --storageEngine=wiredTiger
          --numClientsPerFixture=10
          --mongodSetParameters="{enableTwoPhaseIndexBuild: false}"
      name: jstestfuzz_concurrent_sharded_continuous_stepdown_flow_control

# jstestfuzz replication continuous stepdown with flow control engaged #
- <<: *jstestfuzz_template
  name: jstestfuzz_replication_continuous_stepdown_flow_control_gen
  tags: ["jstestfuzz", "repl", "flow_control"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 30
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_replication_continuous_stepdown
      # TODO(SERVER-44896): Remove enableTwoPhaseIndexBuild from server parameters when two phase
      # index builds work with this flow control setting.
      resmoke_args: >-
          --flowControlTicketOverride=1
          --storageEngine=wiredTiger
          --mongodSetParameters="{logComponentVerbosity: {command: 2}, enableTwoPhaseIndexBuild: false}"
      name: jstestfuzz_replication_continuous_stepdown_flow_control

## jstestfuzz concurrent replication continuous stepdown with flow control engaged ##
- <<: *jstestfuzz_template
  name: jstestfuzz_concurrent_replication_continuous_stepdown_flow_control_gen
  tags: ["jstestfuzz", "common", "repl", "flow_control"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: ${jstestfuzz_concurrent_num_files|10}
      num_tasks: 15
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_replication_continuous_stepdown
      # TODO(SERVER-44896): Remove enableTwoPhaseIndexBuild from server parameters when two phase
      # index builds work with this flow control setting.
      resmoke_args: >-
          --flowControlTicketOverride=10
          --storageEngine=wiredTiger
          --numClientsPerFixture=10
          --mongodSetParameters="{enableTwoPhaseIndexBuild: false}"
      name: jstestfuzz_concurrent_replication_continuous_stepdown_flow_control

## jstestfuzz replica set ##
- <<: *jstestfuzz_template
  name: jstestfuzz_replication_gen
  tags: ["jstestfuzz", "common", "repl"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_replication
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_replication

## jstestfuzz replica set ##
- <<: *jstestfuzz_template
  name: jstestfuzz_replication_multiversion_gen
  tags: ["jstestfuzz_multiversion_gen"]
  commands:
  - func: "generate multiversion tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      suite: jstestfuzz_replication
      is_jstestfuzz: true
      use_multiversion: /data/multiversion
      npm_command: jstestfuzz

## jstestfuzz initial sync replica set ##
- <<: *jstestfuzz_template
  name: jstestfuzz_replication_initsync_gen
  tags: ["jstestfuzz", "initsync"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 8
      num_tasks: 10
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_replication_initsync
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_replication_initsync

## jstestfuzz replica set with logical session ##
- <<: *jstestfuzz_template
  name: jstestfuzz_replication_session_gen
  tags: ["jstestfuzz", "session"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_replication_session
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_replication_session

## jstestfuzz sharded cluster ##
- <<: *jstestfuzz_template
  name: jstestfuzz_sharded_gen
  tags: ["jstestfuzz", "common"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_sharded

## jstestfuzz sharded multiversion cluster ##
- <<: *jstestfuzz_template
  name: jstestfuzz_sharded_multiversion_gen
  tags: [jstestfuzz_multiversion_gen]
  commands:
  - func: "generate multiversion tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      suite: jstestfuzz_sharded
      is_jstestfuzz: true
      use_multiversion: /data/multiversion
      npm_command: jstestfuzz

## jstestfuzz sharded cluster causal consistency ##
- <<: *jstestfuzz_template
  name: jstestfuzz_sharded_causal_consistency_gen
  tags: ["jstestfuzz", "causal"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_causal_consistency
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_sharded_causal_consistency

## jstestfuzz sharded cluster continuous stepdown ##
- <<: *jstestfuzz_template
  name: jstestfuzz_sharded_continuous_stepdown_gen
  tags: ["jstestfuzz"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      num_files: 5
      num_tasks: 10
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_continuous_stepdown
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_sharded_continuous_stepdown

## jstestfuzz sharded cluster with logical session ##
- <<: *jstestfuzz_template
  name: jstestfuzz_sharded_session_gen
  tags: ["jstestfuzz", "session"]
  commands:
  - func: "generate fuzzer tasks"
    vars:
      <<: *jstestfuzz_config_vars
      jstestfuzz_vars: --jsTestsDir ../jstests
      suite: jstestfuzz_sharded_session
      resmoke_args: "--storageEngine=wiredTiger --mongodSetParameters='{logComponentVerbosity: {command: 2}}'"
      name: jstestfuzz_sharded_session

- name: replica_sets_jscore_multiversion_passthrough_gen
  tags: ["multiversion_passthrough"]
  commands:
  - func: "generate multiversion tasks"
    vars:
      suite: replica_sets_jscore_passthrough
      resmoke_args: --storageEngine=wiredTiger
      task_path_suffix: /data/multiversion
      fallback_num_sub_suites: 4

# Check that the mutational fuzzer can parse all JS filess.
- name: lint_fuzzer_sanity_all
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "set task expansion macros"
  - func: "setup jstestfuzz"
  - func: "lint fuzzer sanity all"

## integration test suites ##

- <<: *task_template
  name: aggregation
  tags: ["aggregation", "common"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_disabled_optimization
  tags: ["aggregation", "common"]
  depends_on:
  - name: aggregation
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_disabled_optimization --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_ese
  tags: ["aggregation", "encrypt"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_ese --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_ese_gcm
  tags: ["aggregation", "encrypt", "gcm"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_ese_gcm --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_auth
  tags: ["aggregation", "auth", "common"]
  depends_on:
  - name: aggregation
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_auth --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_facet_unwind_passthrough
  tags: ["aggregation", "unwind"]
  depends_on:
  - name: aggregation
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_facet_unwind_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_mongos_passthrough
  tags: ["aggregation", "no_async"]
  depends_on:
  - name: aggregation
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_mongos_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_one_shard_sharded_collections
  tags: ["aggregation", "no_async"]
  depends_on:
  - name: aggregation
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_one_shard_sharded_collections --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_read_concern_majority_passthrough
  tags: ["aggregation", "read_write_concern"]
  depends_on:
  - name: aggregation
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_read_concern_majority_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: aggregation_sharded_collections_passthrough
  tags: ["aggregation", "common"]
  depends_on:
  - name: aggregation
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=aggregation_sharded_collections_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: audit
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=audit --storageEngine=wiredTiger

- name: auth_gen
  tags: ["auth"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 4

- name: burn_in_tags_gen
  depends_on:
    - name: compile
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "set task expansion macros"
  - func: "set up virtualenv"
    vars:
      pip_dir: ${workdir}/src/etc/pip
  - func: "upload pip requirements"
  - func: "generate burn in tags"
    vars:
      max_revisions: 25
      repeat_tests_secs: 600
      repeat_tests_min: 2
      repeat_tests_max: 1000

- name: auth_audit_gen
  tags: ["auth", "audit"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 4

- <<: *task_template
  name: change_streams
  tags: ["change_streams"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_multiversion_gen
  tags: ["multiversion_passthrough"]
  commands:
  - func: "generate multiversion tasks"
    vars:
      suite: change_streams
      resmoke_args: --storageEngine=wiredTiger
      task_path_suffix: /data/multiversion
      fallback_num_sub_suites: 4

- <<: *task_template
  name: change_streams_mongos_sessions_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_mongos_sessions_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_mongos_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_mongos_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_secondary_reads
  tags: ["change_streams", "secondary_reads"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_secondary_reads --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_sharded_collections_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_sharded_collections_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_sharded_collections_multiversion_passthrough_gen
  tags: ["multiversion_passthrough"]
  depends_on:
  - name: change_streams_multiversion_gen
  commands:
  - func: "generate multiversion tasks"
    vars:
      suite: change_streams_sharded_collections_passthrough
      resmoke_args: --storageEngine=wiredTiger
      task_path_suffix: /data/multiversion
      fallback_num_sub_suites: 4

- <<: *task_template
  name: change_streams_whole_db_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_db_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_whole_db_mongos_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams_mongos_passthrough
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_db_mongos_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_whole_db_secondary_reads_passthrough
  tags: ["change_streams", "secondary_reads"]
  depends_on:
  - name: change_streams_secondary_reads
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_db_secondary_reads_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_whole_db_sharded_collections_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams_sharded_collections_passthrough
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_db_sharded_collections_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_whole_cluster_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_cluster_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_whole_cluster_mongos_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams_mongos_passthrough
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_cluster_mongos_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_whole_cluster_secondary_reads_passthrough
  tags: ["change_streams", "secondary_reads"]
  depends_on:
  - name: change_streams_secondary_reads
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_cluster_secondary_reads_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_whole_cluster_sharded_collections_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams_sharded_collections_passthrough
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_whole_cluster_sharded_collections_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_multi_stmt_txn_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_multi_stmt_txn_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_multi_stmt_txn_mongos_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_multi_stmt_txn_mongos_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: change_streams_multi_stmt_txn_sharded_collections_passthrough
  tags: ["change_streams"]
  depends_on:
  - name: change_streams
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=change_streams_multi_stmt_txn_sharded_collections_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: disk_mobile
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=disk_mobile --storageEngine=mobile
      resmoke_jobs_max: 1

- <<: *task_template
  name: disk_wiredtiger
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=disk_wiredtiger --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: ese
  tags: ["encrypt"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=ese --storageEngine=wiredTiger

- <<: *task_template
  name: failpoints
  tags: ["misc_js"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=failpoints --storageEngine=wiredTiger

- <<: *task_template
  name: failpoints_auth
  tags: ["auth"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=failpoints_auth --storageEngine=wiredTiger

- <<: *task_template
  name: gle_auth
  tags: ["auth", "gle"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=gle_auth --shellWriteMode=legacy --shellReadMode=legacy --excludeWithAnyTags=requires_find_command --storageEngine=wiredTiger

- <<: *task_template
  name: gle_auth_write_cmd
  tags: ["auth", "gle"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=gle_auth --shellWriteMode=commands --storageEngine=wiredTiger

- <<: *task_template
  name: gle_auth_basics_passthrough
  tags: ["auth", "gle"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=gle_auth_basics_passthrough --shellWriteMode=legacy --shellReadMode=legacy --storageEngine=wiredTiger --excludeWithAnyTags=requires_find_command

- <<: *task_template
  name: gle_auth_basics_passthrough_write_cmd
  tags: ["auth", "gle"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=gle_auth_basics_passthrough --shellWriteMode=commands --storageEngine=wiredTiger

- <<: *task_template
  name: integration_tests_standalone
  tags: ["integration", "standalone"]
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "do setup"
  - func: "set up win mount script"
  - func: "generate compile expansions"  # Generate compile expansions needs to be run to mount the shared scons cache.
  - func: "apply compile expansions"
  - func: "scons compile"
    vars:
      targets: integration_tests
      compiling_for_test: true
      bypass_compile: false
      task_compile_flags: >-
        --detect-odr-violations
  - func: "run tests"
    vars:
      resmoke_args: --suites=integration_tests_standalone --storageEngine=wiredTiger

- <<: *task_template
  name: integration_tests_standalone_audit
  tags: ["integration", "audit"]
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "do setup"
  - func: "set up win mount script"
  - func: "generate compile expansions"  # Generate compile expansions needs to be run to mount the shared scons cache.
  - func: "apply compile expansions"
  - func: "scons compile"
    vars:
      targets: integration_tests
      compiling_for_test: true
      bypass_compile: false
      task_compile_flags: >-
        --detect-odr-violations
  - func: "run tests"
    vars:
      resmoke_args: --suites=integration_tests_standalone_audit --storageEngine=wiredTiger

- <<: *task_template
  name: integration_tests_replset
  tags: ["integration"]
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "do setup"
  - func: "set up win mount script"
  - func: "generate compile expansions"  # Generate compile expansions needs to be run to mount the shared scons cache.
  - func: "apply compile expansions"
  - func: "scons compile"
    vars:
      targets: integration_tests
      compiling_for_test: true
      bypass_compile: false
      task_compile_flags: >-
        --detect-odr-violations
  - func: "run tests"
    vars:
      resmoke_args: --suites=integration_tests_replset --storageEngine=wiredTiger

- <<: *task_template
  name: integration_tests_sharded
  tags: ["integration", "sharded"]
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "do setup"
  - func: "set up win mount script"
  - func: "generate compile expansions"  # Generate compile expansions needs to be run to mount the shared scons cache.
  - func: "apply compile expansions"
  - func: "scons compile"
    vars:
      targets: integration_tests
      compiling_for_test: true
      bypass_compile: false
      task_compile_flags: >-
        --detect-odr-violations
  - func: "run tests"
    vars:
      resmoke_args: --suites=integration_tests_sharded --storageEngine=wiredTiger

- <<: *task_template
  name: external_auth
  commands:
  - func: "do setup"
  - command: shell.exec
    params:
      shell: bash
      script: |
        set -o errexit
        set -o verbose
        ${activate_virtualenv}

        # Not all git get project calls clone into ${workdir}/src so we allow
        # callers to tell us where the pip requirements files are.
        pip_dir="${pip_dir}"
        if [[ -z $pip_dir ]]; then
          # Default to most common location
          pip_dir="${workdir}/src/etc/pip"
        fi

        # Same as above we have to use quotes to preserve the
        # Windows path separator
        external_auth_txt="$pip_dir/external-auth-requirements.txt"
        python -m pip install -r "$external_auth_txt"
  - func: "run tests"
    vars:
      resmoke_args: --suites=external_auth --excludeWithAnyTags=requires_domain_controller --storageEngine=wiredTiger

- <<: *task_template
  name: sharding_gle_auth_basics_passthrough
  tags: ["auth", "gle"]
  depends_on:
  - name: gle_auth_basics_passthrough
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharding_gle_auth_basics_passthrough --shellWriteMode=legacy --shellReadMode=legacy --storageEngine=wiredTiger --excludeWithAnyTags=requires_find_command

- <<: *task_template
  name: sharding_gle_auth_basics_passthrough_write_cmd
  tags: ["auth", "gle"]
  depends_on:
  - name: gle_auth_basics_passthrough_write_cmd
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharding_gle_auth_basics_passthrough --shellWriteMode=commands --storageEngine=wiredTiger

- <<: *task_template
  name: jsCore
  tags: ["jscore", "common"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core --storageEngine=wiredTiger

- <<: *task_template
  name: jsCore_ese
  tags: ["jscore", "encrypt"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core_ese --storageEngine=wiredTiger

- <<: *task_template
  name: jsCore_ese_gcm
  tags: ["jscore", "encrypt", "gcm"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core_ese_gcm --storageEngine=wiredTiger

- <<: *task_template
  name: jsCore_compatibility
  tags: ["jscore", "common", "compat"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core --shellReadMode=legacy --shellWriteMode=compatibility --storageEngine=wiredTiger --excludeWithAnyTags=requires_find_command

- <<: *task_template
  name: jsCore_auth
  tags: ["jscore", "auth", "common"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core_auth

- <<: *task_template
  name: jsCore_minimum_batch_size
  tags: ["jscore"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core_minimum_batch_size --storageEngine=wiredTiger

- <<: *task_template
  name: jsCore_op_query
  tags: ["jscore"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core_op_query --storageEngine=wiredTiger

- <<: *task_template
  name: jsCore_txns
  tags: ["jscore", "common", "txns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core_txns --storageEngine=wiredTiger

- <<: *task_template
  name: jsCore_txns_large_txns_format
  tags: ["jscore", "txns", "multi_oplog"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=core_txns_large_txns_format --storageEngine=wiredTiger

- <<: *task_template
  name: sharded_jscore_txns
  tags: ["sharding", "jscore", "txns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharded_jscore_txns --storageEngine=wiredTiger

- <<: *task_template
  name: sharded_jscore_txns_without_snapshot
  tags: ["sharding", "wo_snapshot", "jscore"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharded_jscore_txns --storageEngine=wiredTiger --excludeWithAnyTags=uses_snapshot_read_concern

- <<: *task_template
  name: sharded_jscore_txns_sharded_collections
  tags: ["sharding", "jscore", "txns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharded_jscore_txns_sharded_collections --storageEngine=wiredTiger

- <<: *task_template
  name: causally_consistent_jscore_txns_passthrough
  tags: ["causally_consistent"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=causally_consistent_jscore_txns_passthrough --storageEngine=wiredTiger

- name: sharded_causally_consistent_jscore_txns_passthrough_gen
  tags: ["sharding", "jscore", "causally_consistent", "txns"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 1

- name: sharded_causally_consistent_jscore_txns_passthrough_without_snapshot_gen
  tags: ["sharding", "wo_snapshot", "causally_consistent", "jscore"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: sharded_causally_consistent_jscore_txns_passthrough
      resmoke_args: --storageEngine=wiredTiger --excludeWithAnyTags=uses_snapshot_read_concern
      fallback_num_sub_suites: 1

- <<: *task_template
  name: sharded_collections_causally_consistent_jscore_txns_passthrough
  tags: ["sharding", "jscore", "causally_consistent", "txns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharded_collections_causally_consistent_jscore_txns_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_jscore_passthrough
  tags: ["replica_sets", "common", "san", "large"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_jscore_passthrough --storageEngine=wiredTiger

- name: replica_sets_jscore_passthrough_gen
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 1

- <<: *task_template
  name: replica_sets_large_txns_format_jscore_passthrough
  tags: ["replica_sets", "multi_oplog", "large", "non_maj_read", "san"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_large_txns_format_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_multi_stmt_txn_jscore_passthrough
  tags: ["replica_sets", "large"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_multi_stmt_txn_jscore_passthrough --storageEngine=wiredTiger

- name: replica_sets_multi_stmt_txn_stepdown_jscore_passthrough_gen
  tags: ["replica_sets", "non_maj_read"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 15

- <<: *task_template
  name: replica_sets_multi_stmt_txn_kill_primary_jscore_passthrough
  tags: ["replica_sets", "non_maj_read"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_multi_stmt_txn_kill_primary_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_multi_stmt_txn_terminate_primary_jscore_passthrough
  tags: ["replica_sets", "non_maj_read"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_multi_stmt_txn_terminate_primary_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_initsync_jscore_passthrough
  tags: ["replica_sets", "san", "large"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_initsync_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_initsync_static_jscore_passthrough
  tags: ["replica_sets", "san", "large"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_initsync_static_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_kill_primary_jscore_passthrough
  tags: ["replica_sets", "large", "non_maj_read"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_kill_primary_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_terminate_primary_jscore_passthrough
  tags: ["replica_sets", "large", "non_maj_read"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_terminate_primary_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: replica_sets_kill_secondaries_jscore_passthrough
  tags: ["replica_sets", "san", "large"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets_kill_secondaries_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: mongosTest
  tags: ["misc_js", "non_mobile", "non_read_maj"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=mongos_test

- <<: *task_template
  name: multiversion_auth
  tags: ["auth", "multiversion"]
  commands:
  - func: "do setup"
  - func: "do multiversion setup"
  - func: "run tests"
    vars:
      task_path_suffix: /data/multiversion
      resmoke_args: --suites=multiversion_auth --storageEngine=wiredTiger

- <<: *task_template
  name: multiversion
  commands:
  - func: "do setup"
  - func: "do multiversion setup"
  - func: "run tests"
    vars:
      task_path_suffix: /data/multiversion
      resmoke_args: --suites=multiversion --storageEngine=wiredTiger

- name: unittest_shell_hang_analyzer_gen
  tags: ["misc_js", "sharded"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: unittest_shell_hang_analyzer
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 1

- name: noPassthrough_gen
  tags: ["misc_js"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: no_passthrough
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 12

- name: noPassthroughWithMongod_gen
  tags: ["misc_js"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: no_passthrough_with_mongod
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 5

- <<: *task_template
  name: bulk_gle_passthrough
  tags: ["auth", "gle"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=bulk_gle_passthrough --storageEngine=wiredTiger

- name: slow1_gen
  tags: ["misc_js", "non_win_dbg", "non_mobile"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 4
      resmoke_jobs_max: 1

- <<: *task_template
  name: serial_run
  tags: ["misc_js", "non_win_dbg"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=serial_run --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: sharded_collections_jscore_passthrough
  tags: ["sharding", "jscore"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharded_collections_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: sharded_collections_jscore_multiversion_passthrough_gen
  tags: ["multiversion_passthrough"]
  commands:
  - func: "generate multiversion tasks"
    vars:
      suite: sharded_collections_jscore_passthrough
      resmoke_args: --storageEngine=wiredTiger
      task_path_suffix: /data/multiversion
      fallback_num_sub_suites: 4

- <<: *task_template
  name: sharding_jscore_passthrough
  tags: ["sharding", "jscore", "common"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharding_jscore_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: sharding_jscore_multiversion_passthrough_gen
  tags: ["multiversion_passthrough"]
  commands:
  - func: "generate multiversion tasks"
    vars:
      suite: sharding_jscore_passthrough
      resmoke_args: --storageEngine=wiredTiger
      task_path_suffix: /data/multiversion
      fallback_num_sub_suites: 4

- <<: *task_template
  name: sharding_jscore_op_query_passthrough
  tags: ["sharding", "jscore"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharding_jscore_op_query_passthrough --storageEngine=wiredTiger

- name: sharding_jscore_passthrough_wire_ops_gen
  tags: ["sharding", "jscore"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: sharding_jscore_passthrough
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger --shellReadMode=legacy --shellWriteMode=compatibility --excludeWithAnyTags=requires_find_command
      fallback_num_sub_suites: 11

- <<: *task_template
  name: sharded_multi_stmt_txn_jscore_passthrough
  tags: ["sharding", "jscore", "multi_stmt"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sharded_multi_stmt_txn_jscore_passthrough --storageEngine=wiredTiger

- name: multi_shard_multi_stmt_txn_jscore_passthrough_gen
  tags: ["multi_shard", "multi_stmt", "common"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 28
      resmoke_jobs_max: 0  # No cap on number of jobs.

- name: multi_shard_local_read_write_multi_stmt_txn_jscore_passthrough_gen
  tags: ["multi_shard", "common"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 21

- name: multi_stmt_txn_jscore_passthrough_with_migration_gen
  tags: ["multi_stmt"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 19

- name: multi_shard_multi_stmt_txn_kill_primary_jscore_passthrough_gen
  tags: ["multi_shard"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 48

- name: multi_shard_multi_stmt_txn_stepdown_primary_jscore_passthrough_gen
  tags: ["multi_shard"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 37

- name: parallel_gen
  tags: ["misc_js", "non_mobile"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 2
      resmoke_jobs_max: 1

- <<: *task_template
  name: parallel_compatibility
  tags: ["misc_js", "non_mobile"]
  depends_on:
  - name: jsCore_compatibility
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=parallel --shellReadMode=legacy --shellWriteMode=compatibility --storageEngine=wiredTiger --excludeWithAnyTags=requires_find_command
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency
  tags: ["concurrency", "common"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_replication
  tags: ["concurrency", "common", "repl"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_replication_multiversion_gen
  tags: [multiversion_passthrough]
  commands:
  - func: "generate multiversion tasks"
    vars:
      suite: concurrency_replication
      resmoke_args: --storageEngine=wiredTiger
      task_path_suffix: /data/multiversion
      fallback_num_sub_suites: 4

- <<: *task_template
  name: concurrency_replication_causal_consistency
  tags: ["concurrency", "repl", "large"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication_causal_consistency --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_replication_multi_stmt_txn
  tags: ["concurrency", "common", "repl", "txn"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication_multi_stmt_txn --storageEngine=wiredTiger
      resmoke_jobs_max: 1

  # TODO: SERVER-35964 revert the addition of UBSAN concurrency_replication suites.
- <<: *task_template
  name: concurrency_replication_ubsan
  tags: ["concurrency", "ubsan"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication_ubsan --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_replication_causal_consistency_ubsan
  tags: ["concurrency", "ubsan"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication_causal_consistency_ubsan --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_replication_multi_stmt_txn_ubsan
  tags: ["concurrency", "ubsan"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication_multi_stmt_txn_ubsan --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_replication_wiredtiger_cursor_sweeps
  tags: ["concurrency", "repl"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication_wiredtiger_cursor_sweeps --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_replication_wiredtiger_eviction_debug
  tags: ["concurrency", "repl", "debug_only"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_replication_wiredtiger_eviction_debug --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_replication
  tags: ["concurrency", "common", "read_concern_maj"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_replication --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_replication_multiversion_gen
  tags: [multiversion_passthrough]
  commands:
  - func: "generate multiversion tasks"
    vars:
      suite: concurrency_sharded_replication
      resmoke_args: --storageEngine=wiredTiger
      task_path_suffix: /data/multiversion
      fallback_num_sub_suites: 4

- <<: *task_template
  name: concurrency_sharded_replication_with_balancer
  tags: ["concurrency", "common", "read_concern_maj"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_replication_with_balancer --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_replication_no_txns
  tags: ["concurrency", "no_txns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_replication --excludeWithAnyTags=uses_transactions --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_replication_no_txns_with_balancer
  tags: ["concurrency", "no_txns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_replication_with_balancer --excludeWithAnyTags=uses_transactions --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_clusterwide_ops_add_remove_shards
  tags: ["concurrency", "common", "read_concern_maj"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_clusterwide_ops_add_remove_shards --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- name: concurrency_sharded_causal_consistency_gen
  tags: ["concurrency"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 3
      use_large_distro: "true"
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_causal_consistency_and_balancer
  tags: ["concurrency"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_causal_consistency_and_balancer --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_with_stepdowns
  tags: ["concurrency", "stepdowns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_with_stepdowns --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_with_stepdowns_and_balancer
  tags: ["concurrency", "stepdowns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_with_stepdowns_and_balancer --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_terminate_primary_with_balancer
  tags: ["concurrency", "stepdowns", "kill_terminate"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_terminate_primary_with_balancer --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_kill_primary_with_balancer
  tags: ["concurrency", "stepdowns", "kill_terminate"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_kill_primary_with_balancer --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_multi_stmt_txn
  tags: ["concurrency"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_multi_stmt_txn --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_multi_stmt_txn_with_balancer
  tags: ["concurrency"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_multi_stmt_txn_with_balancer --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_local_read_write_multi_stmt_txn
  tags: ["concurrency"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_local_read_write_multi_stmt_txn --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_local_read_write_multi_stmt_txn_with_balancer
  tags: ["concurrency"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_local_read_write_multi_stmt_txn_with_balancer --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_multi_stmt_txn_with_stepdowns
  tags: ["concurrency", "stepdowns"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_multi_stmt_txn_with_stepdowns --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_multi_stmt_txn_terminate_primary
  tags: ["concurrency", "stepdowns", "kill_terminate"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_multi_stmt_txn_terminate_primary --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_sharded_multi_stmt_txn_kill_primary
  tags: ["concurrency", "stepdowns", "kill_terminate"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_sharded_multi_stmt_txn_kill_primary --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_simultaneous
  tags: ["concurrency", "common"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_simultaneous --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_simultaneous_replication
  tags: ["concurrency", "common"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_simultaneous_replication --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_simultaneous_replication_wiredtiger_cursor_sweeps
  tags: ["concurrency", "repl"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_simultaneous_replication_wiredtiger_cursor_sweeps --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: concurrency_simultaneous_replication_wiredtiger_eviction_debug
  tags: ["concurrency", "repl", "debug_only"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=concurrency_simultaneous_replication_wiredtiger_eviction_debug --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: read_concern_linearizable_passthrough
  tags: ["read_write_concern", "linearize", "large"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=read_concern_linearizable_passthrough --storageEngine=wiredTiger

- name: read_concern_majority_passthrough_gen
  tags: ["read_write_concern"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 10

- <<: *task_template
  name: write_concern_majority_passthrough
  tags: ["read_write_concern", "large", "write"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=write_concern_majority_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: cwrwc_passthrough
  tags: ["read_write_concern", "large", "write"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=cwrwc_passthrough --storageEngine=wiredTiger

- name: cwrwc_rc_majority_passthrough_gen
  tags: ["read_write_concern"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 10

- <<: *task_template
  name: cwrwc_wc_majority_passthrough
  tags: ["read_write_concern", "large", "write"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=cwrwc_wc_majority_passthrough --storageEngine=wiredTiger

- name: secondary_reads_passthrough_gen
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 12

- <<: *task_template
  name: replica_sets
  tags: ["replica_sets", "san", "large"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=replica_sets --storageEngine=wiredTiger

- name: replica_sets_ese_gen
  tags: ["replica_sets", "encrypt", "san"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 15

- name: replica_sets_ese_gcm_gen
  tags: ["replica_sets", "encrypt", "san", "gcm"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 15

- name: replica_sets_auth_gen
  tags: ["replica_sets", "common", "san", "auth"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 14

- name: replica_sets_large_txns_format_gen
  tags: ["replica_sets", "multi_oplog", "san"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 15

- <<: *task_template
  name: replica_sets_multiversion
  tags: ["random_multiversion_replica_sets"]
  commands:
  - func: "git get project"
  - func: "do setup"
  - func: "do multiversion setup"
  - command: shell.exec
    params:
      working_dir: src
      script: |
        set -o errexit
        set -o verbose

        ${activate_virtualenv}
        $python buildscripts/evergreen_gen_multiversion_tests.py generate-exclude-files --suite=replica_sets_multiversion --task-path-suffix=/data/multiversion --is-generated-suite=false
  - func: "run tests"
    vars:
      task_path_suffix: /data/multiversion
      resmoke_args: --suites=replica_sets_multiversion --storageEngine=wiredTiger

- <<: *task_template
  name: sasl
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=sasl --storageEngine=wiredTiger

- name: sharding_gen
  tags: ["sharding", "common"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 32

- name: sharding_multiversion_gen
  tags: ["random_multiversion_replica_sets"]
  commands:
  - func: "generate randomized multiversion tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 32
      use_multiversion: /data/multiversion
      suite: sharding_multiversion

- name: sharding_rs_matching_disabled_gen
  tags: ["sharding", "common"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 32

- name: sharding_rs_matching_match_busiest_node_gen
  tags: ["sharding", "common"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 32

- name: sharding_csrs_continuous_config_stepdown_gen
  tags: ["sharding", "common", "csrs"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: sharding_continuous_config_stepdown
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 25

- name: sharding_ese_gen
  tags: ["sharding", "encrypt"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 32

- name: sharding_ese_gcm_gen
  tags: ["sharding", "encrypt", "gcm"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 32

- name: sharding_op_query_gen
  tags: ["sharding", "common", "op_query"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: sharding
      use_large_distro: "true"
      resmoke_args: --shellReadMode=legacy --storageEngine=wiredTiger --excludeWithAnyTags=requires_find_command
      fallback_num_sub_suites: 31

- name: sharding_auth_gen
  tags: ["sharding", "auth"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 31

- name: sharding_auth_audit_gen
  tags: ["auth", "audit"]
  depends_on:
  - name: sharding_auth_gen
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: sharding_auth
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 31

- name: sharding_last_stable_mongos_and_mixed_shards_gen
  tags: ["sharding", "common", "multiversion"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      use_large_distro: "true"
      use_multiversion: /data/multiversion
      resmoke_args: ""
      fallback_num_sub_suites: 24

- <<: *task_template
  name: snmp
  commands:
  - func: "do setup"
  - func: "do snmp setup"
  - func: "run tests"
    vars:
      snmp_config_path: SNMPCONFPATH=snmpconf
      resmoke_args: --suites=snmp --storageEngine=wiredTiger

- name: ssl_gen
  tags: ["encrypt", "ssl"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 5

- name: sslSpecial_gen
  tags: ["encrypt", "ssl"]
  commands:
  - func: "generate resmoke tasks"
    vars:
      suite: ssl_special
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 3

- <<: *task_template
  name: jsCore_decimal
  tags: ["jscore", "common", "decimal"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=decimal --storageEngine=wiredTiger

- <<: *task_template
  name: read_only
  tags: ["read_only"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=read_only --storageEngine=wiredTiger

- <<: *task_template
  name: read_only_sharded
  tags: ["read_only"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=read_only_sharded --storageEngine=wiredTiger

- <<: *task_template
  name: session_jscore_passthrough
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=session_jscore_passthrough --storageEngine=wiredTiger

- name: causally_consistent_jscore_passthrough_gen
  tags: ["causally_consistent"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 14

- name: causally_consistent_jscore_passthrough_auth_gen
  tags: ["causally_consistent"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 15

- name: sharded_causally_consistent_jscore_passthrough_gen
  tags: ["causally_consistent"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 15

- name: retryable_writes_jscore_passthrough_gen
  tags: ["retry"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      use_large_distro: "true"
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 9

- name: logical_session_cache_replication_default_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache", "repl"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 17

- name: logical_session_cache_replication_100ms_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache", "repl"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 17

- name: logical_session_cache_replication_1sec_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache", "one_sec", "repl"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 16

- name: logical_session_cache_replication_10sec_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache", "repl"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 16

- name: logical_session_cache_sharding_default_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 12

- name: logical_session_cache_sharding_100ms_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 48

- name: logical_session_cache_sharding_1sec_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache", "one_sec"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 20

- name: logical_session_cache_sharding_10sec_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 14

- name: logical_session_cache_standalone_default_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 5

- name: logical_session_cache_standalone_100ms_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 5

- name: logical_session_cache_standalone_1sec_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache", "one_sec"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 5

- name: logical_session_cache_standalone_10sec_refresh_jscore_passthrough_gen
  tags: ["logical_session_cache"]
  depends_on:
  - name: jsCore
  commands:
  - func: "generate resmoke tasks"
    vars:
      depends_on: jsCore
      resmoke_args: --storageEngine=wiredTiger
      fallback_num_sub_suites: 5

- <<: *task_template
  name: retryable_writes_jscore_stepdown_passthrough
  tags: ["retry"]
  depends_on:
  - name: jsCore
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=retryable_writes_jscore_stepdown_passthrough --storageEngine=wiredTiger

- <<: *task_template
  name: watchdog_wiredtiger
  tags: ["watchdog"]
  commands:
  - func: "do setup"
  - func: "do watchdog setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=watchdog --storageEngine=wiredTiger
      resmoke_jobs_max: 1

# This is a separate task because it is only supported on Ubuntu 16.04+ which are not inmemory builders
- <<: *task_template
  name: watchdog_inmemory
  tags: ["watchdog"]
  commands:
  - func: "do setup"
  - func: "do watchdog setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=watchdog --storageEngine=inMemory
      resmoke_jobs_max: 1

- <<: *task_template
  name: free_monitoring
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=free_monitoring --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: client_encrypt
  tags: ["ssl", "encrypt"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=client_encrypt --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: fle
  tags: ["encrypt"]
  commands:
  - func: "do setup"
  - func: "load aws test credentials"
  - func: "run tests"
    vars:
      resmoke_args: --suites=fle --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: ocsp
  tags: ["ssl", "ocsp"]
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=ocsp
      resmoke_jobs_max: 1

- <<: *task_template
  name: jsonSchema
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=json_schema --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- name: idl_tests
  depends_on:
  - name: compile
  commands:
  - func: "do setup"
  - func: "run idl tests"

- name: buildscripts_test
  depends_on: []
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "set task expansion macros"
  - func: "set up virtualenv"
  - func: "upload pip requirements"
  - func: "get buildnumber"
  - func: "set up credentials"
  - func: "run tests"
    vars:
      resmoke_args: --suites=buildscripts_test
      resmoke_jobs_max: 1

- name: package
  depends_on:
  - name: compile
  commands:
    - func: "fetch artifacts"
    - func: "set up remote credentials"
      vars:
        private_key_file: ~/.ssh/kitchen.pem
        private_key_remote: ${kitchen_private_key}
        aws_key_remote: ${kitchen_aws_key}
        aws_secret_remote: ${kitchen_aws_secret}
    - func: "run kitchen"

- name: publish_packages
  tags: ["publish"]
  # This should prevent this task from running in patch builds, where we
  # don't want to publish packages.
  patchable: false
  stepback: false
  # Same dependencies as "push" below
  depends_on:
  - name: compile
  - name: jsCore
  - name: dbtest
  - name: replica_sets_jscore_passthrough
  commands:
    - func: "fetch artifacts"
    - func: "fetch packages"
    - func: "apply compile expansions"
    - func: "set up remote credentials"
      vars:
        aws_key_remote: ${repo_aws_key}
        aws_secret_remote: ${repo_aws_secret}
    - func: "set up notary client credentials"
    - command: shell.exec
      params:
        working_dir: src
        script: |
          . ./notary_env.sh

          set -o errexit
          set -o verbose

          CURATOR_RELEASE=${curator_release|"latest"}
          curl -L -O http://boxes.10gen.com/build/curator/curator-dist-rhel70-$CURATOR_RELEASE.tar.gz
          tar -zxvf curator-dist-rhel70-$CURATOR_RELEASE.tar.gz
          ./curator repo --config ./etc/repo_config.yaml --distro ${packager_distro} --edition ${repo_edition} --version ${version} --arch ${packager_arch} --packages repo

- name: push
  tags: ["publish"]
  patchable: false
  depends_on:
  - name: jsCore
  - name: dbtest
  - name: replica_sets_jscore_passthrough
  stepback: false
  commands:
    - func: "fetch artifacts"
    - func: "fetch packages"
    - func: "fetch binaries"
    # Fetch the shell
    - command: s3.get
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        remote_file: ${mongo_shell}
        bucket: mciuploads
        local_file: src/mongo-shell.tgz
    # Fetch mongocryptd
    - command: s3.get
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        remote_file: ${mongo_cryptd}
        bucket: mciuploads
        local_file: src/mongo-cryptd.tgz
        build_variants: *mongocryptd_variants
    # Fetch mh
    - command: s3.get
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        remote_file: ${mh_archive}
        bucket: mciuploads
        local_file: src/mh.tgz
        build_variants: *mh_variants
    # Fetch the sources (on relevant variants only)
    - command: s3.get
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        remote_file: ${project}/${build_variant}/${revision}/sources/mongo-src-${build_id}.${ext|tgz}
        bucket: mciuploads
        local_file: src/distsrc.${ext|tgz}
        build_variants: [rhel70]
    - func: "apply compile expansions"
    - func: "fetch debugsymbols archive"
    - func: "set up remote credentials"
      vars:
        aws_key_remote: ${repo_aws_key}
        aws_secret_remote: ${repo_aws_secret}
    - func: "set up notary client credentials"
    - command: shell.exec
      params:
        working_dir: src
        script: |
          . ./notary_env.sh

          set -o errexit
          set -o verbose

          mv mongo-binaries.tgz mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}
          mv mongo-shell.tgz mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}
          mv mongo-cryptd.tgz mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz} || true
          mv mh.tgz mh-${push_name}-${push_arch}-${suffix}.${ext|tgz} || true
          mv mongo-debugsymbols.tgz mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz} || true
          mv distsrc.${ext|tgz} mongodb-src-${src_suffix}.${ext|tar.gz} || true
          /usr/bin/find build/ -type f | grep msi$ | xargs -I original_filename cp original_filename mongodb-${push_arch}-${suffix}.msi || true

          /usr/local/bin/notary-client.py --key-name "server-4.4" --auth-token-file ${workdir}/src/signing_auth_token --comment "Evergreen Automatic Signing ${revision} - ${build_variant} - ${branch_name}" --notary-url http://notary-service.build.10gen.cc:5000 --skip-missing mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz} mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz} mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz} mongodb-${push_arch}-${suffix}.msi mongodb-src-${src_suffix}.${ext|tar.gz} mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}

    # Put the binaries tarball/zipfile
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}
    # Put the shell tarball/zipfile
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}
    # Put the cryptd tarball/zipfile
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}
        build_variants: *mongocryptd_variants
    # Put the mh tarball/zipfile
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mh-${push_name}-${push_arch}-${suffix}.${ext|tgz}
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mh-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}
        build_variants: *mh_variants
    # Put the source tarball
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-src-${src_suffix}.${ext|tar.gz}
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}
        build_variants: [rhel70]

    # Put the debug symbols
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        permissions: public-read
        local_file: src/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}
        bucket: build-push-testing
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}
        optional: true

    # Put the binaries tarball signature
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sig
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sig

    # Put the shell tarball signature
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sig
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sig

    # Put the cryptd tarball signature
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sig
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sig
        build_variants: *mongocryptd_variants

    # Put the source tarball signature
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-src-${src_suffix}.${ext|tar.gz}.sig
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.sig
        build_variants: [rhel70]

    # Put the debug symbols signature
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        permissions: public-read
        local_file: src/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.sig
        bucket: build-push-testing
        content_type: ${content_type|application/gzip}
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.sig
        optional: true

    # Put the binaries tarball sha1
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha1
        aws_key: ${aws_key}
        permissions: public-read
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha1

    # Put the shell tarball sha1
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha1
        aws_key: ${aws_key}
        permissions: public-read
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha1

    # Put the cryptd tarball sha1
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha1
        aws_key: ${aws_key}
        permissions: public-read
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha1
        build_variants: *mongocryptd_variants

    # Put the source tarball sha1
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-src-${src_suffix}.${ext|tar.gz}.sha1
        aws_key: ${aws_key}
        permissions: public-read
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.sha1
        build_variants: [rhel70]

    # Put the debug symbols sha1
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        aws_key: ${aws_key}
        permissions: public-read
        local_file: src/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.sha1
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.sha1
        optional: true

    # Put the binaries tarball sha256
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha256
        permissions: public-read
        aws_key: ${aws_key}
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha256

    # Put the shell tarball sha256
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha256
        permissions: public-read
        aws_key: ${aws_key}
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha256

    # Put the cryptd tarball sha256
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha256
        permissions: public-read
        aws_key: ${aws_key}
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha256
        build_variants: *mongocryptd_variants

    # Put the source tarball sha256
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-src-${src_suffix}.${ext|tar.gz}.sha256
        permissions: public-read
        aws_key: ${aws_key}
        bucket: build-push-testing
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.sha256
        build_variants: [rhel70]

    # Put the debug symbols sha256
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.sha256
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.sha256
        optional: true

    # Put the binaries tarball md5
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.md5
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.md5

    # Put the shell tarball md5
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.md5
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.md5

    # Put the cryptd tarball md5
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.md5
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.md5
        build_variants: *mongocryptd_variants

    # Put the source tarball md5
    - command: s3.put
      params:
        aws_secret: ${aws_secret}
        local_file: src/mongodb-src-${src_suffix}.${ext|tar.gz}.md5
        aws_key: ${aws_key}
        bucket: build-push-testing
        permissions: public-read
        content_type: text/plain
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.md5
        build_variants: [rhel70]

    # Put the debug symbols md5
    - command: s3.put
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.md5
        bucket: build-push-testing
        content_type: text/plain
        permissions: public-read
        remote_file: ${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.md5
        optional: true

    - command: s3Copy.copy
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        s3_copy_files:
            #Binaries
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}', 'bucket': '${push_bucket}'}}

            #Shell
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}', 'bucket': '${push_bucket}'}}

            #Cryptd
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}', 'bucket': '${push_bucket}'},
               'build_variants': *mongocryptd_variants}

            # MH
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mh-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mh-${push_name}-${push_arch}-${suffix}.${ext|tgz}', 'bucket': '${push_bucket}'},
               'build_variants': *mh_variants}

            #Source tarball
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}', 'bucket': 'build-push-testing'},
               'destination': {'path': 'src/mongodb-src-${src_suffix}.${ext|tar.gz}', 'bucket': '${push_bucket}'},
               'build_variants': ['rhel70']}

            #Binaries Signature
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sig', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sig', 'bucket': '${push_bucket}'}}

            #Shell Signature
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sig', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sig', 'bucket': '${push_bucket}'}}

            #Cryptd Signature
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sig', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sig', 'bucket': '${push_bucket}'},
               'build_variants': *mongocryptd_variants}

            #Source tarball signature
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.sig', 'bucket': 'build-push-testing'},
               'destination': {'path': 'src/mongodb-src-${src_suffix}.${ext|tar.gz}.sig', 'bucket': '${push_bucket}'},
               'build_variants': ['rhel70']}

            #SHA1 for binaries
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha1', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha1', 'bucket': '${push_bucket}'}}

            #SHA1 for shell
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha1', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha1', 'bucket': '${push_bucket}'}}

            #SHA1 for cryptd
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha1', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha1', 'bucket': '${push_bucket}'},
               'build_variants': *mongocryptd_variants}

            #SHA1 for source tarball
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.sha1', 'bucket': 'build-push-testing'},
               'destination': {'path': 'src/mongodb-src-${src_suffix}.${ext|tar.gz}.sha1', 'bucket': '${push_bucket}'},
               'build_variants': ['rhel70']}

            #SHA256 for binaries
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha256', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha256', 'bucket': '${push_bucket}'}}

            #SHA256 for shell
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha256', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha256', 'bucket': '${push_bucket}'}}

            #SHA256 for cryptd
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.sha256', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.sha256', 'bucket': '${push_bucket}'},
               'build_variants': *mongocryptd_variants}

            #SHA256 for source tarball
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.sha256', 'bucket': 'build-push-testing'},
               'destination': {'path': 'src/mongodb-src-${src_suffix}.${ext|tar.gz}.sha256', 'bucket': '${push_bucket}'},
               'build_variants': ['rhel70']}

            #MD5 for binaries
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.md5', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-${suffix}.${ext|tgz}.md5', 'bucket': '${push_bucket}'}}

            #MD5 for shell
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-shell-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.md5', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-shell-${push_name}-${push_arch}-${suffix}.${ext|tgz}.md5', 'bucket': '${push_bucket}'}}

            #MD5 for cryptd
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}-${task_id}.${ext|tgz}.md5', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-cryptd-${push_name}-${push_arch}-${suffix}.${ext|tgz}.md5', 'bucket': '${push_bucket}'},
               'build_variants': *mongocryptd_variants}

            #MD5 for source tarball
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-src-${src_suffix}-${task_id}.${ext|tar.gz}.md5', 'bucket': 'build-push-testing'},
               'destination': {'path': 'src/mongodb-src-${src_suffix}.${ext|tar.gz}.md5', 'bucket': '${push_bucket}'},
               'build_variants': ['rhel70']}

    # Debug symbols are not created for all variants and the copy is optional.
    - command: s3Copy.copy
      params:
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        optional: true
        s3_copy_files:
            #Debug Symbols
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}', 'bucket': '${push_bucket}'}}

            #Debug Symbols Signature
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.sig', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.sig', 'bucket': '${push_bucket}'}}

            #SHA1 for debug symbols
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.sha1', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.sha1', 'bucket': '${push_bucket}'}}

            #SHA256 for debugsymbols
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.sha256', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.sha256', 'bucket': '${push_bucket}'}}

            #MD5 for debugsymbols
            - {'source': {'path': '${push_path}-STAGE/${push_name}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}-${task_id}.${ext|tgz}.md5', 'bucket': 'build-push-testing'},
               'destination': {'path': '${push_path}/mongodb-${push_name}-${push_arch}-debugsymbols-${suffix}.${ext|tgz}.md5', 'bucket': '${push_bucket}'}}

- <<: *task_template
  name: search_beta
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=search_beta --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: search_beta_auth
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=search_beta_auth --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- <<: *task_template
  name: search_beta_ssl
  commands:
  - func: "do setup"
  - func: "run tests"
    vars:
      resmoke_args: --suites=search_beta_ssl --storageEngine=wiredTiger
      resmoke_jobs_max: 1

- name: commit_queue_placeholder
  commands:
  - command: shell.exec
    params:
      script: |
        echo "Commit Queue Placeholder"
        exit 0

- name: validate_commit_message
  commands:
  - command: manifest.load
  - func: "git get project"
  - func: "set task expansion macros"
  - func: "set up virtualenv"
  - command: shell.exec
    type: test
    params:
      working_dir: src
      shell: bash
      script: |
        set -o verbose
        set -o errexit
        if [ "${is_commit_queue}" = "true" ]; then
          # Since `commit_message` is an evergreen expansion, we need a way to ensure we
          # properly deal with any special characters that could cause issues (like "). To
          # do this, we will write it out to a file, then read that file into a variable.
          cat > commit_message.txt <<END_OF_COMMIT_MSG
        ${commit_message}
        END_OF_COMMIT_MSG

          commit_message_content=$(cat commit_message.txt)

          ${activate_virtualenv}
          $python buildscripts/validate_commit_message.py "$commit_message_content"
        fi


#######################################
#             Task Groups             #
#######################################
task_groups:
- <<: *compile_task_group_template
  name: compile_TG
  tasks:
  - compile
- <<: *compile_task_group_template
  name: compile_core_tools_TG
  tasks:
  - compile_core_tools
- <<: *compile_task_group_template
  name: compile_ninja_TG
  tasks:
  - compile_ninja
  teardown_task:
    - command: s3.put
      params:
        optional: true
        aws_key: ${aws_key}
        aws_secret: ${aws_secret}
        local_file: src/all.build.ninja
        remote_file: ${project}/${build_variant}/${revision}/artifacts/all.${build_id}.build.ninja
        bucket: mciuploads
        permissions: public-read
        content_type: text/plain
        display_name: build.ninja
- <<: *compile_task_group_template
  name: server_discovery_and_monitoring_json_test_TG
  tasks:
  - server_discovery_and_monitoring_json_test
- <<: *compile_task_group_template
  name: dbtest_TG
  tasks:
  - dbtest
- <<: *compile_task_group_template
  name: libfuzzertests_TG
  tasks:
  - compile_libfuzzertests
  - libfuzzertests
- <<: *compile_task_group_template
  name: compile_all_run_unittests_TG
  tasks:
  - compile
  - unittests
  - dbtest
  - compile_all

- name: embedded_sdk_build_and_test
  setup_group_can_fail_task: true
  max_hosts: 1
  setup_group:
    - command: manifest.load
    - func: "git get project"
    - func: "get buildnumber"
    - func: "set up credentials"
    - func: "set task expansion macros"
    - func: "set up virtualenv"
    - func: "upload pip requirements"
    - func: "set up win mount script"
    - func: "generate compile expansions"
  teardown_group:
    - func: "umount shared scons directory"
  setup_task:
    - func: "set task expansion macros"
    - func: "apply compile expansions"
  teardown_task:
  tasks:
  - "embedded_sdk_build_cdriver"
  - "embedded_sdk_install_dev"
  - "embedded_sdk_s3_put"
  - "embedded_sdk_install_tests"
  - "embedded_sdk_tests_s3_put"
  - "embedded_sdk_run_tests"
  - "embedded_sdk_s3_put_latest"
  - "embedded_sdk_tests_s3_put_latest"

- <<: *stitch_support_task_group_template
  name: stitch_support_lib_build_and_archive
  tags: ["stitch"]
  tasks:
    - "stitch_support_create_lib"
- <<: *stitch_support_task_group_template
  name: stitch_support_lib_build_and_test
  tags: ["stitch"]
  max_hosts: 1
  tasks:
    - "stitch_support_install_tests"
    - "stitch_support_run_tests"

#######################################
#               Modules               #
#######################################
# if a module is added and to be added to the manifest
# be sure to add the module to git.get_project revisions parameter
modules:
- name: enterprise
  repo: git@github.com:10gen/mongo-enterprise-modules.git
  prefix: src/mongo/db/modules
  branch: master

- name: wtdevelop
  repo: git@github.com:wiredtiger/wiredtiger.git
  prefix: src/third_party
  branch: develop

#######################################
#            Buildvariants            #
#######################################

buildvariants:

- name: ubuntu1804-debug-aubsan-lite
  display_name: "! {A,UB}SAN Enterprise Ubuntu 18.04 DEBUG"
  batchtime: 60 # 1 hour
  modules:
  - enterprise
  run_on:
  - ubuntu1804-build
  stepback: true
  expansions:
    # We need llvm-symbolizer in the PATH for ASAN for clang-3.7 or later.
    variant_path_suffix: /opt/mongodbtoolchain/v3/bin
    lang_environment: LANG=C
    san_options: UBSAN_OPTIONS="print_stacktrace=1" LSAN_OPTIONS="suppressions=etc/lsan.suppressions:report_objects=1" ASAN_OPTIONS=detect_leaks=1:check_initialization_order=true:strict_init_order=true:abort_on_error=1:disable_coredump=0:handle_abort=1
    compile_flags: --variables-files=etc/scons/mongodbtoolchain_v3_clang.vars --dbg=on --opt=on --allocator=system --sanitize=undefined,address --ssl -j$(grep -c ^processor /proc/cpuinfo) --nostrip
    resmoke_jobs_factor: 0.3  # Avoid starting too many mongod's under {A,UB}SAN build.
    tooltags: "ssl"
    build_mongoreplay: true
    hang_analyzer_dump_core: false
    scons_cache_scope: shared
  tasks:
  - name: compile_all_run_unittests_TG
  - name: server_discovery_and_monitoring_json_test_TG
  - name: jsCore
  - name: jsCore_txns
